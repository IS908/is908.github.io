[{"title":"Bro脚本编写","url":"/2016/11/22/Bro/","content":"\n本文英文原版链接为：https://www.bro.org/sphinx/scripting/index.html#understanding-bro-scripts ，仅供参考。\n\nUnderstanding Bro ScriptsBro 包括事件驱动的脚本语言，为组织扩展和自定义Bro的功能提供了主要方法。事实上，Bro生成的所有输出事实上都是由Bro脚本生成的。将Bro作为幕后处理和生成事件的实体几乎更容易，而Bro脚本语言是我们使用者可以实现通信的媒介。Bro脚本有效地通知Bro，如果有一个类型的事件，我们定义，然后让我们有关于连接的信息，所以我们可以执行一些功能。例如，ssl.log文件由Bro脚本生成，该脚本遍历整个证书链，并在证书链中的任何步骤无效时发出通知。整个过程是通过告诉Bro，如果它看到一个服务器或客户端问题SSL HELLO消息，我们想知道有关该连接的信息。\n\n\n通过查看完整的脚本将其分解为可识别的组件来了解Bro的脚本语言通常是最容易的。在这个例子中，我们将看到Bro如何检查从网络流量提取的各种文件的SHA1哈希与Team Cymru Malware哈希注册表。 Cymru Malware Hash注册表的一部分包括使用格式 .malware.hash.cymru.com在域上执行主机查找的功能，其中是文件的SHA1哈希。团队Cymru也填充他们的DNS响应的TXT记录与“首见”时间戳和数字“检测率”。要了解的重要方面是Bro已经通过Files框架生成文件的哈希，但它是脚本detect-MHR.bro，负责生成适当的DNS查找，解析响应，并生成是否适用的通知。\ndetect-MHR.bro##! Detect file downloads that have hash values matching files in Team##! Cymru&#x27;s Malware Hash Registry (http://www.team-cymru.org/Services/MHR/).@load base/frameworks/files@load base/frameworks/notice@load frameworks/files/hash-all-filesmodule TeamCymruMalwareHashRegistry;export &#123;    redef enum Notice::Type += &#123;        ## The hash value of a file transferred over HTTP matched in the        ## malware hash registry.        Match    &#125;;    ## File types to attempt matching against the Malware Hash Registry.    const match_file_types = /application\\/x-dosexec/ |                             /application\\/vnd.ms-cab-compressed/ |                             /application\\/pdf/ |                             /application\\/x-shockwave-flash/ |                             /application\\/x-java-applet/ |                             /application\\/jar/ |                             /video\\/mp4/ &amp;redef;    ## The Match notice has a sub message with a URL where you can get more    ## information about the file. The %s will be replaced with the SHA-1    ## hash of the file.    const match_sub_url = &quot;https://www.virustotal.com/en/search/?query=%s&quot; &amp;redef;    ## The malware hash registry runs each malware sample through several    ## A/V engines.  Team Cymru returns a percentage to indicate how    ## many A/V engines flagged the sample as malicious. This threshold    ## allows you to require a minimum detection rate.    const notice_threshold = 10 &amp;redef;&#125;function do_mhr_lookup(hash: string, fi: Notice::FileInfo)    &#123;    local hash_domain = fmt(&quot;%s.malware.hash.cymru.com&quot;, hash);    when ( local MHR_result = lookup_hostname_txt(hash_domain) )        &#123;        # Data is returned as &quot;&lt;dateFirstDetected&gt; &lt;detectionRate&gt;&quot;        local MHR_answer = split_string1(MHR_result, / /);        if ( |MHR_answer| == 2 )            &#123;            local mhr_detect_rate = to_count(MHR_answer[1]);            if ( mhr_detect_rate &gt;= notice_threshold )                &#123;                local mhr_first_detected = double_to_time(to_double(MHR_answer[0]));                local readable_first_detected = strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, mhr_first_detected);                local message = fmt(&quot;Malware Hash Registry Detection rate: %d%%  Last seen: %s&quot;, mhr_detect_rate, readable_first_detected);                local virustotal_url = fmt(match_sub_url, hash);                # We don&#x27;t have the full fa_file record here in order to                # avoid the &quot;when&quot; statement cloning it (expensive!).                local n: Notice::Info = Notice::Info($note=Match, $msg=message, $sub=virustotal_url);                Notice::populate_file_info2(fi, n);                NOTICE(n);                &#125;            &#125;        &#125;    &#125;event file_hash(f: fa_file, kind: string, hash: string)    &#123;    if ( kind == &quot;sha1&quot; &amp;&amp; f?$info &amp;&amp; f$info?$mime_type &amp;&amp;          match_file_types in f$info$mime_type )        do_mhr_lookup(hash, Notice::create_file_info(f));    &#125;\n\n看起来，Bro脚本有三个不同的部分。首先，有一个没有缩进的基本层，其中库通过 @load 包含在脚本中，命名空间用模块定义。这之后是一个缩进和格式化的部分，作为解释脚本命名空间一部分提供（导出）的自定义变量。最后，有一个第二个缩进和格式化部分，描述了对特定事件（event file_hash）采取的指令。如果你不理解脚本的每一部分，不要慌;我们将在后面的章节中介绍脚本的基础知识。\ndetect-MHR.bro@load base/frameworks/files@load base/frameworks/notice@load frameworks/files/hash-all-files\n\n脚本的第一部分由@load指令组成，其处理在加载的相应目录中的 load.bro 脚本。在编写Bro脚本时，@load指令通常被认为是良好的做法，甚至是唯一的推荐方式，以确保它们可以自己使用。虽然在Bro的完全生产部署中这些额外的资源可能不会被加载，但是当你有了一定的Bro脚本编写经验时，可以尝试进行相应加载的优化。当你还是个新手，这个级别的粒度可能先不要尝试了。 @load指令用于确保 Files framework, Notice framework 和用于哈希所有文件的脚本 已由Bro加载。\ndetect-MHR.broexport &#123;    redef enum Notice::Type += &#123;        ## The hash value of a file transferred over HTTP matched in the        ## malware hash registry.        Match    &#125;;    ## File types to attempt matching against the Malware Hash Registry.    const match_file_types = /application\\/x-dosexec/ |                             /application\\/vnd.ms-cab-compressed/ |                             /application\\/pdf/ |                             /application\\/x-shockwave-flash/ |                             /application\\/x-java-applet/ |                             /application\\/jar/ |                             /video\\/mp4/ &amp;redef;    ## The Match notice has a sub message with a URL where you can get more    ## information about the file. The %s will be replaced with the SHA-1    ## hash of the file.    const match_sub_url = &quot;https://www.virustotal.com/en/search/?query=%s&quot; &amp;redef;    ## The malware hash registry runs each malware sample through several    ## A/V engines.  Team Cymru returns a percentage to indicate how    ## many A/V engines flagged the sample as malicious. This threshold    ## allows you to require a minimum detection rate.    const notice_threshold = 10 &amp;redef;&#125;\n\nexport部分重新定义了一个枚举常量，它描述了我们将使用Notice框架生成的通知类型。 Bro允许重新定义常量，这可能看起来反直觉。我们将在后面的章节中更深入地介绍常量，而当下，将它们看作只能在Bro开始运行之前更改的变量。所列出的通知类型允许使用NOTICE函数生成TeamCymruMalwareHashRegistry :: Match类型的通知，如下一节所述。通知允许Bro在其默认日志类型之外生成某种额外的通知。通常，此额外通知以电子邮件的形式生成并发送到预配置的地址，但可根据部署的需要进行更改。export部分完成了几个常数的定义，列出了我们要匹配的文件的类型和我们感兴趣的检测阈值的最小百分比。\n直到现在，脚本只做了一些简单的设置。在下一节中，脚本开始定义接收给定事件的指令。\ndetect-MHR.brofunction do_mhr_lookup(hash: string, fi: Notice::FileInfo)    &#123;    local hash_domain = fmt(&quot;%s.malware.hash.cymru.com&quot;, hash);    when ( local MHR_result = lookup_hostname_txt(hash_domain) )        &#123;        # Data is returned as &quot;&lt;dateFirstDetected&gt; &lt;detectionRate&gt;&quot;        local MHR_answer = split_string1(MHR_result, / /);        if ( |MHR_answer| == 2 )            &#123;            local mhr_detect_rate = to_count(MHR_answer[1]);            if ( mhr_detect_rate &gt;= notice_threshold )                &#123;                local mhr_first_detected = double_to_time(to_double(MHR_answer[0]));                local readable_first_detected = strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, mhr_first_detected);                local message = fmt(&quot;Malware Hash Registry Detection rate: %d%%  Last seen: %s&quot;, mhr_detect_rate, readable_first_detected);                local virustotal_url = fmt(match_sub_url, hash);                # We don&#x27;t have the full fa_file record here in order to                # avoid the &quot;when&quot; statement cloning it (expensive!).                local n: Notice::Info = Notice::Info($note=Match, $msg=message, $sub=virustotal_url);                Notice::populate_file_info2(fi, n);                NOTICE(n);                &#125;            &#125;        &#125;    &#125;event file_hash(f: fa_file, kind: string, hash: string)    &#123;    if ( kind == &quot;sha1&quot; &amp;&amp; f?$info &amp;&amp; f$info?$mime_type &amp;&amp;          match_file_types in f$info$mime_type )        do_mhr_lookup(hash, Notice::create_file_info(f));    &#125;\n\n脚本的工作负载包含在file_hash的事件处理程序中。 file_hash事件允许脚本访问与Bro的文件分析框架生成了散列的文件相关联的信息。事件处理程序将文件本身作为f传递，将摘要算法的类型用作分类，并将散列生成为散列。在file_hash事件处理程序中，有一个if语句用于检查正确类型的散列，在这种情况下是一个SHA1散列。它还检查我们定义为感兴趣的MIME类型，如常量match_file_types中定义的。对比表达式f $ info $ mime_type，它使用$ dereference运算符来检查变量f $ info中的值mime_type。如果整个表达式的值为true，那么将调用一个辅助函数来完成其余的工作。在该函数中，局部变量被定义为保存由与.malware.hash.cymru.com连接的SHA1哈希组成的字符串;此值将是在恶意软件散列注册表中查询的域。脚本的其余部分包含在when块中。简而言之，当Bro需要执行异步操作（例如DNS查找）时，使用when块，以确保不会影响性能。 when块执行DNS TXT查找，并将结果存储在本地变量MHR_result中。实际上，继续处理该事件，并且在接收由lookup_hostname_txt返回的值时，执行when块。 when 块将返回的字符串拆分为第一次检测到恶意软件的日期的一部分，并通过分割文本空间并存储本地表变量中返回的值来检测检测率。在do_mhr_lookup函数中，如果split1返回的表有两个条目，表示成功拆分，我们使用适当的转换函数将检测日期存储在mhr_first_detected中，并将速率存储在mhr_detect_rate中。从这一点上，Bro知道它已经看到一个文件传输，其中有一个已经被团队Cymru Malware哈希注册表看到的散列，脚本的其余部分致力于产生通知。检测时间被处理为字符串表示并存储在readable_first_detected中。然后，脚本将检测率与之前定义的notice_threshold进行比较。如果检测率足够高，脚本将创建简明的通知描述并将其存储在消息变量中。它还创建了一个可能的URL，以检查样本与virustotal.com的数据库，并调用NOTICE将相关信息移交给Notice framework。在大约几十行代码中，Bro提供了一个难以实现和部署与其他产品的惊人的实用程序。事实上，Bro声明在这么少的行中做这是一个误导;在Bro中，幕后有真正大量的事情，但它是让分析师已简洁明确的方式访问这些基础层的一种脚本语言。\nThe Event Queue and Event Handlers (事件队列和事件处理程序)Bro的脚本语言是事件驱动的，这是一种来自大多数脚本语言的齿轮变化（组件化？），大多数用户可凭借之前的脚本经验。 Bro中的脚本关键在于处理Bro生成的事件，因为它处理网络流量，通过这些事件更改数据结构的状态，以及根据提供的信息做出决策。这种脚本编写方法通常会对从程序或功能语言转到Bro的用户造成混乱，但是一旦初始冲击造成的困惑消失，每次曝光变得更加清楚。Bro的核心行为是将事件放置到有序的“事件队列”中，从而允许事件处理程序以先到先服务的方式处理它们。实际上，这是Bro的核心功能，因为没有编写脚本来对事件执行离散操作，将几乎没有可用输出。因此，对事件队列，生成的事件以及事件处理程序处理这些事件的方式的基本理解不仅是学习编写Bro的脚本，而且是理解Bro本身的基础。熟悉Bro生成的特定事件是构建使用Bro脚本的体系的一大步。 Bro生成的大多数事件在内置函数文件或.bif文件中定义，它们也是联机事件文档的基础。这些内嵌注释使用Broxygen编译为在线文档系统。无论从头开始一个脚本还是阅读和维护别人的脚本，内置的事件定义都是可用的，是一个很好的资源。对于2.0版本，Bro开发人员花费大量精力来组织和记录每个事件。这种努力导致组织的内置函数文件，而且每个条目包含描述性事件名称，传递给事件的参数，以及对函数使用的简明解释。\nBro_DNS.events.bif.bro## Generated for DNS requests. For requests with multiple queries, this event## is raised once for each.#### See `Wikipedia &lt;http://en.wikipedia.org/wiki/Domain_Name_System&gt;`__ for more## information about the DNS protocol. Bro analyzes both UDP and TCP DNS## sessions.#### c: The connection, which may be UDP or TCP depending on the type of the##    transport-layer session being analyzed.#### msg: The parsed DNS message header.#### query: The queried name.#### qtype: The queried resource record type.#### qclass: The queried resource record class.#### .. bro:see:: dns_AAAA_reply dns_A_reply dns_CNAME_reply dns_EDNS_addl##    dns_HINFO_reply dns_MX_reply dns_NS_reply dns_PTR_reply dns_SOA_reply##    dns_SRV_reply dns_TSIG_addl dns_TXT_reply dns_WKS_reply dns_end##    dns_full_request dns_mapping_altered dns_mapping_lost_name dns_mapping_new_name##    dns_mapping_unverified dns_mapping_valid dns_message dns_query_reply##    dns_rejected non_dns_request dns_max_queries dns_session_timeout dns_skip_addl##    dns_skip_all_addl dns_skip_all_auth dns_skip_authglobal dns_request: event(c: connection , msg: dns_msg , query: string , qtype: count , qclass: count );\n\n上面是事件dns_request的文档的一部分（前面的链接指向生成的文档）。它的组织使得文档，注释和参数列表在Bro使用的实际事件定义之前。 当 Bro检测到发起方发出的DNS请求时，它会发出此事件，因此任何数量的脚本都可以访问Bro数据及事件。在此示例中，Bro不仅传递DNS请求的消息，查询，查询类型和查询类，还传递用于连接本身的记录。\nThe Connection Record Data Type (连接记录数据类型)在Bro定义的所有事件中，绝大多数事件都被传递连接记录数据类型，实际上，使其成为许多脚本解决方案的主干。连接记录本身，正如我们稍后会看到的，是大量的嵌套数据类型，用于通过其生命周期跟踪连接上的状态。让我们通过选择一个适当的事件，生成一些输出到标准输出和剖析连接记录的过程，以获得它的概述。稍后我们将更详细地介绍数据类型。尽管Bro能够进行分组级处理，但其优势在于始发者和响应者之间的连接的上下文。因此，为连接生命周期的主要部分定义了事件，你将从下面的小型连接相关事件中看到。\nevent.bif.bro## Generated for every new connection. This event is raised with the first## packet of a previously unknown connection. Bro uses a flow-based definition## of &quot;connection&quot; here that includes not only TCP sessions but also UDP and## ICMP flows.global new_connection: event(c: connection );## Generated when a TCP connection timed out. This event is raised when## no activity was seen for an interval of at least## :bro:id:`tcp_connection_linger`, and either one endpoint has already## closed the connection or one side never became active.global connection_timeout: event(c: connection );## Generated when a connection&#x27;s internal state is about to be removed from## memory. Bro generates this event reliably once for every connection when it## is about to delete the internal state. As such, the event is well-suited for## script-level cleanup that needs to be performed for every connection.  This## event is generated not only for TCP sessions but also for UDP and ICMP## flows.global connection_state_remove: event(c: connection );\n在列出的事件中，将使我们最好地了解连接记录数据类型的事件将是connection_state_remove。正如官方在线文档所述，Bro在决定从内存中删除此事件之前生成此事件，从而有效地忘记了该事件。让我们来看看一个简单的示例脚本，它将输出单个连接的连接记录。\nconnection_record_01.bro@load base/protocols/connevent connection_state_remove(c: connection)    &#123;    print c;    &#125;\n再次，我们从@load开始，这次导入Package：base &#x2F; protocols &#x2F; conn脚本，它提供对一般信息和连接状态的跟踪和记录。我们处理connection_state_remove事件，只是打印传递给它的参数的内容。对于这个例子，我们将以“裸模式”运行Bro，它只加载最少数量的脚本以保持可操作性，并且不必为正在运行的脚本加载所需的脚本。虽然裸模式是并入Bro的低级功能，在这种情况下，我们将使用它来演示Bro的不同功能如何添加更多的关于连接的信息层。这将让我们有机会看到连接记录的内容，而不会过度填充。\n# bro -b -r http/get.trace connection_record_01.bro[id=[orig_h=141.142.228.5, orig_p=59856/tcp, resp_h=192.150.187.43, resp_p=80/tcp], orig=[size=136, state=5, num_pkts=7, num_bytes_ip=512, flow_label=0, l2_addr=c8:bc:c8:96:d2:a0], resp=[size=5007, state=5, num_pkts=7, num_bytes_ip=5379, flow_label=0, l2_addr=00:10:db:88:d2:ef], start_time=1362692526.869344, duration=0.211484, service=&#123;&#125;, history=ShADadFf, uid=CHhAvVGS1DHFjwGM9, tunnel=&lt;uninitialized&gt;, vlan=&lt;uninitialized&gt;, inner_vlan=&lt;uninitialized&gt;, conn=[ts=1362692526.869344, uid=CHhAvVGS1DHFjwGM9, id=[orig_h=141.142.228.5, orig_p=59856/tcp, resp_h=192.150.187.43, resp_p=80/tcp], proto=tcp, service=&lt;uninitialized&gt;, duration=0.211484, orig_bytes=136, resp_bytes=5007, conn_state=SF, local_orig=&lt;uninitialized&gt;, local_resp=&lt;uninitialized&gt;, missed_bytes=0, history=ShADadFf, orig_pkts=7, orig_ip_bytes=512, resp_pkts=7, resp_ip_bytes=5379, tunnel_parents=&#123;&#125;], extract_orig=F, extract_resp=F, thresholds=&lt;uninitialized&gt;]\n从输出中可以看出，连接记录在打印时是混乱的。定期查看填充的连接记录有助于了解其字段之间的关系，并有助于构建一个用于访问脚本中数据的参考框架。Bro大量使用嵌套数据结构来存储从连接分析中收集的状态和信息作为完整单元。要分解这个信息集合，你必须使用Bro的字段分隔符$。例如，发起主机由 c$id$orig_h 引用，如果给出叙述涉及 orig_h，其是 id 的成员，其是被称为c的被传递到事件处理程序中的数据结构的成员。鉴于响应程序端口 c$id$resp_p 是 80&#x2F;tcp，很可能Bro的基本HTTP脚本可以进一步填充连接记录。让我们加载 base&#x2F;protocols&#x2F;http 脚本并检查我们的脚本的输出。Bro使用美元符号作为其字段分隔符，并且在连接记录的输出和脚本中取消引用的变量的正确格式之间存在直接关联。在上面脚本的输出中，在括号之间收集信息组，这些信息对应于Bro脚本中的 $-delimiter。\nconnection_record_02.bro@load base/protocols/conn@load base/protocols/httpevent connection_state_remove(c: connection)    &#123;    print c;    &#125;\nbro脚本:\n# bro -b -r http/get.trace connection_record_02.bro[id=[orig_h=141.142.228.5, orig_p=59856/tcp, resp_h=192.150.187.43, resp_p=80/tcp], orig=[size=136, state=5, num_pkts=7, num_bytes_ip=512, flow_label=0, l2_addr=c8:bc:c8:96:d2:a0], resp=[size=5007, state=5, num_pkts=7, num_bytes_ip=5379, flow_label=0, l2_addr=00:10:db:88:d2:ef], start_time=1362692526.869344, duration=0.211484, service=&#123;&#125;, history=ShADadFf, uid=CHhAvVGS1DHFjwGM9, tunnel=&lt;uninitialized&gt;, vlan=&lt;uninitialized&gt;, inner_vlan=&lt;uninitialized&gt;, conn=[ts=1362692526.869344, uid=CHhAvVGS1DHFjwGM9, id=[orig_h=141.142.228.5, orig_p=59856/tcp, resp_h=192.150.187.43, resp_p=80/tcp], proto=tcp, service=&lt;uninitialized&gt;, duration=0.211484, orig_bytes=136, resp_bytes=5007, conn_state=SF, local_orig=&lt;uninitialized&gt;, local_resp=&lt;uninitialized&gt;, missed_bytes=0, history=ShADadFf, orig_pkts=7, orig_ip_bytes=512, resp_pkts=7, resp_ip_bytes=5379, tunnel_parents=&#123;&#125;], extract_orig=F, extract_resp=F, thresholds=&lt;uninitialized&gt;, http=[ts=1362692526.939527, uid=CHhAvVGS1DHFjwGM9, id=[orig_h=141.142.228.5, orig_p=59856/tcp, resp_h=192.150.187.43, resp_p=80/tcp], trans_depth=1, method=GET, host=bro.org, uri=/download/CHANGES.bro-aux.txt, referrer=&lt;uninitialized&gt;, version=1.1, user_agent=Wget/1.14 (darwin12.2.0), request_body_len=0, response_body_len=4705, status_code=200, status_msg=OK, info_code=&lt;uninitialized&gt;, info_msg=&lt;uninitialized&gt;, tags=&#123;&#125;, username=&lt;uninitialized&gt;, password=&lt;uninitialized&gt;, capture_password=F, proxied=&lt;uninitialized&gt;, range_request=F, orig_fuids=&lt;uninitialized&gt;, orig_filenames=&lt;uninitialized&gt;, orig_mime_types=&lt;uninitialized&gt;, resp_fuids=[FakNcS1Jfe01uljb3], resp_filenames=&lt;uninitialized&gt;, resp_mime_types=[text/plain], current_entity=&lt;uninitialized&gt;, orig_mime_depth=1, resp_mime_depth=1], http_state=[pending=&#123;&#125;, current_request=1, current_response=1, trans_depth=1]]\n添加 base&#x2F;protocols&#x2F;http 脚本将填充连接记录的 http &#x3D; [ ] 成员。虽然Bro在后台做了大量的工作，但它是通常所谓的细化的实时决断的“脚本”。如果我们继续以“裸模式”运行，我们可以通过@load语句缓慢地添加基础结构。例如，是否为 @load base&#x2F;frameworks&#x2F;logging，Bro将在当前工作目录中为我们生成 conn.log 和 http.log。如上所述，包括适当的@load语句不仅是良好的实践，而且还可以帮助指示在脚本中使用哪些功能。花一秒钟运行不带-b标志的脚本，并在将所有Bro的功能应用于跟踪文件时检查输出。\nData Types and Data Structures (数据类型和数据结构)Scope （作用域）在开始探索Bro的本地数据类型和数据结构之前，了解Bro中可用的不同级别的可用范围以及在脚本中使用它们的适当时间非常重要。 Bro中变量的声明有两种形式。变量可以使用或不使用SCOPE名称中的定义来声明：TYPE或SCOPE name &#x3D; EXPRESSION;如果EXPRESSION评估为与TYPE相同的类型，则每个都会产生相同的结果。关于使用哪种类型的声明的可由个人偏好和可读性决定。\ndata_type_declaration.broevent bro_init()    &#123;    local a: int;    a = 10;    local b = 10;    if ( a == b )    print fmt(&quot;A: %d, B: %d&quot;, a, b);    &#125;\nGlobal Variables （全局变量）当需要跟踪变量的状态时，使用全局变量，这并不令人惊讶。虽然有一些注意事项，当脚本使用全局范围声明变量时，该脚本正在授予从其他脚本访问该变量的权限。但是，当脚本使用module关键字为脚本提供命名空间时，必须更多地注意全局变量的声明，以确保预期的结果。当在具有命名空间的脚本中声明全局时，有两种可能的情况。情景一，变量只在命名空间的上下文中可用。在这种情况下，同一命名空间中的其他脚本将有权访问声明的变量，而使用不同命名空间或没有命名空间的脚本将无法访问该变量；情景二，如果在 export { … } 块中声明了一个全局变量，该变量通过命名约定 MODULE :: variable_name 可用于任何其他脚本。下面的声明来自 policy&#x2F;protocols&#x2F;conn&#x2F;known-hosts.bro 脚本，并声明一个名为known_hosts的变量作为已知命名空间内的唯一IP地址的全局集合，并将其导出以在Known命名空间外部使用。如果我们要使用known_hosts变量，我们可以通过Known :: known_hosts来访问它。\nknown-hosts.bromodule Known;export &#123;    global known_hosts: set[addr] &amp;create_expire=1day &amp;synchronized &amp;redef;&#125;\n上面的示例还使用 export { … } 块。当在脚本中使用module关键字时，声明的变量被称为在该模块的 “namespace”。其中作为一个全局变量，当它没有在一个模块中声明时，它的名称可以被访问，一个模块中声明的全局变量必须被导出，然后通过MODULE_NAME :: VARIABLE_NAME访问。如上面的例子，我们将能够通过Known :: known_hosts在一个单独的脚本变量中访问known_hosts，因为known_hosts在Known命名空间下的导出块中被声明为一个全局变量。\nConstants （常量）Bro也使用常量，由const关键字表示。与全局变量不同，常量只能在解析时使用＆redef属性设置或更改。之后（在运行时）常量是不可更改的。在大多数情况下，可重定义的常量在Bro脚本中用作配置选项的容器。例如，记录从HTTP流解密的密码的配置选项存储在 HTTP :: default_capture_password 中，如下面从 base&#x2F;protocols&#x2F;http&#x2F;main.bro 中摘录的摘录所示。\nhttp_main.bromodule HTTP;export &#123;    ## This setting changes if passwords used in Basic-Auth are captured or not.    const default_capture_password = F &amp;redef;&#125;\n因为常量是用＆redef属性声明的，如果我们需要在全局上打开这个选项，我们可以通过在我们的 site&#x2F;local.bro 文件中添加下面一行来启动Bro。\ndata_type_const_simple.bro@load base/protocols/httpredef HTTP::default_capture_password = T;\n虽然可重定义的常量的想法可能是奇怪的，但常量只能在解析时改变的约束仍然保留即使使用＆redef属性。在下面的代码片段中，通过端口索引的字符串表被声明为常量，然后通过redef语句将两个值添加到表中。然后在bro_init事件中打印该表。如果我们尝试改变事件处理程序中的表，则Bro将告知用户错误，并且脚本将失败。\ndata_type_const.broconst port_list: table[port] of string &amp;redef;redef port_list += &#123; [6666/tcp] = &quot;IRC&quot;&#125;;redef port_list += &#123; [80/tcp] = &quot;WWW&quot; &#125;;event bro_init()    &#123;    print port_list;    &#125;\n执行bro脚本：\n# bro -b data_type_const.bro&#123;[80/tcp] = WWW,[6666/tcp] = IRC&#125;\nLocal Variables （局部变量）尽管全局变量和常量通过各种手段在脚本中广泛可用，但当变量使用局部作用域定义时，它的可用性仅限于它所声明的事件或函数的主体。局部变量倾向于用于仅在特定范围内需要的值，并且一旦脚本的处理超出该范围并且不再使用，则该变量被删除。 Bro维护本地化的名称与全局可见的名称分开，下面将举例说明。\ndata_type_local.brofunction add_two(i: count): count    &#123;    local added_two = i+2;    print fmt(&quot;i + 2 = %d&quot;, added_two);    return added_two;    &#125;event bro_init()    &#123;    local test = add_two(10);    &#125;\n脚本执行事件处理程序bro_init，它反过来调用参数为10的函数add_two（i：count）。一旦Bro进入add_two函数，它将提供一个局部范围的变量 added_two 来保存 i + 2 . add_two 函数然后打印added_two 变量的值，并将其值返回到 bro_init 事件处理程序。此时，变量added_two已经超出范围，并且不再存在，而值12仍在使用并存储在本地作用域变量测试中。当Bro完成处理bro_init函数时，名为test的变量不再在作用域中，因为没有对值12的其他引用，该值也被删除。\nData Structures在没有覆盖Bro中可用的数据结构的情况下，很难以实用的方式讨论Bro的数据类型。当在数据结构内部使用时，揭示了数据类型的一些更有趣的特性，但是考虑到数据结构由数据类型组成，它相当快地转化为“鸡和蛋”问题。因此，我们将从鸟瞰图引入数据类型，然后再进入数据结构，并从中更全面地探索数据类型。下表显示了Bro中使用的原子类型，如果你有一些脚本编写经验，前四个应该看起来很熟悉，而剩下的六个在其他语言中不太常见。网络安全监控平台的脚本语言具有相当强大的以网络为中心的数据类型，并且在这里记录这些数据类型可能会为您节省一个重新发明轮子的时间。\nData Type\tDescriptionint\t64 bit signed integercount\t64 bit unsigned integerdouble\tdouble precision floating precisionbool\tboolean(T/F)addr\tIP address, IPv4 and IPv6port\ttransport layer portsubnet\tCIDR subnet masktime\tabsolute epoch timeinterval\ta time intervalpattern\tregular expression\nSetsBro中的 sets 用于存储相同数据类型的唯一元素。实质上，你可以将它们视为“一组唯一的整数”或“一组唯一的IP地址”。虽然集合的声明可能因收集的数据类型而异，但集合将始终包含唯一元素，集合中的元素将始终具有相同的数据类型。这样的要求使得集合数据类型对于已经自然唯一的信息（例如端口或IP地址）是完美的。下面的代码片段显示了局部范围集合的显式和隐式声明。\ndata_struct_set_declaration.broevent bro_init()    &#123;    local ssl_ports: set[port];    local non_ssl_ports = set( 23/tcp, 80/tcp, 143/tcp, 25/tcp );    &#125;\n如你所见，使用格式SCOPE var_name：set [TYPE]声明集合。使用add和delete语句实现在集合中添加和删除元素。一旦你有元素插入到集合中，你可能需要迭代该集合或测试集合中的成员资格，这两个都由 in 运算符覆盖。在迭代一个集合的情况下，结合使用 for 语句和 in 运算符将允许你顺序处理集合的每个元素，如下所示。\ndata_struct_set_declaration.brofor ( i in ssl_ports )        print fmt(&quot;SSL Port: %s&quot;, i);for ( i in non_ssl_ports )        print fmt(&quot;Non-SSL Port: %s&quot;, i);\n这里，for语句循环存储临时变量i中的每个元素的集合的内容。对于for循环的每次迭代，选择下一个元素。由于集合不是有序数据类型，因此不能保证元素作为for循环过程的顺序。要测试集合中的成员资格，in语句可以与if语句组合，以返回true或false值。如果条件中的确切元素已经在集合中，则条件返回true，并且正文执行。 in语句也可以被否定！运算符创建条件的逆。虽然我们可以重写相应的行，如同（！（587 &#x2F; tcp in ssl_ports））尽量避免使用这个结构;相反，取消in运算符本身。虽然功能是相同的，使用！in是更有效的，以及一个更自然的结构，这将有助于您的脚本的可读性。\ndata_struct_set_declaration.bro# Check for SMTPS if ( 587/tcp !in ssl_ports )    add ssl_ports[587/tcp];\n您可以在下面看到完整的脚本及其输出。\ndata_struct_set_declaration.broevent bro_init()    &#123;    local ssl_ports: set[port];    local non_ssl_ports = set( 23/tcp, 80/tcp, 143/tcp, 25/tcp );    # SSH    add ssl_ports[22/tcp];    # HTTPS    add ssl_ports[443/tcp];    # IMAPS    add ssl_ports[993/tcp];    # Check for SMTPS     if ( 587/tcp !in ssl_ports )    add ssl_ports[587/tcp];    for ( i in ssl_ports )    print fmt(&quot;SSL Port: %s&quot;, i);    for ( i in non_ssl_ports )    print fmt(&quot;Non-SSL Port: %s&quot;, i);    &#125;\n执行data_struct_set_declaration.bro脚本：\n# bro data_struct_set_declaration.broSSL Port: 22/tcpSSL Port: 443/tcpSSL Port: 587/tcpSSL Port: 993/tcpNon-SSL Port: 80/tcpNon-SSL Port: 25/tcpNon-SSL Port: 143/tcpNon-SSL Port: 23/tcp\nTablesBro中的表是键到值或yield的映射。虽然值不必是唯一的，但表中的每个键必须是唯一的，以保留键与值的一对一映射。\ndata_struct_table_declaration.broevent bro_init()    &#123;    # Declaration of the table.    local ssl_services: table[string] of port;    # Initialize the table.    ssl_services = table([&quot;SSH&quot;] = 22/tcp, [&quot;HTTPS&quot;] = 443/tcp);    # Insert one key-yield pair into the table.    ssl_services[&quot;IMAPS&quot;] = 993/tcp;    # Check if the key &quot;SMTPS&quot; is not in the table.    if ( &quot;SMTPS&quot; !in ssl_services )    ssl_services[&quot;SMTPS&quot;] = 587/tcp;    # Iterate over each key in the table.    for ( k in ssl_services )    print fmt(&quot;Service Name:  %s - Common Port: %s&quot;, k, ssl_services[k]);    &#125;\n执行data_struct_table_declaration.bro脚本：\n# bro data_struct_table_declaration.broService Name:  SSH - Common Port: 22/tcpService Name:  HTTPS - Common Port: 443/tcpService Name:  SMTPS - Common Port: 587/tcpService Name:  IMAPS - Common Port: 993/tcp\n在本例中，我们编译了一个启用SSL的服务及其公共端口的表。表的显式声明和构造函数在两个不同的行上，并且布置keys（strings）的数据类型和yields（port）的数据类型，然后填充一些示例键值对。您还可以使用表访问器将一个键值对插入表中。当在表上使用 in 运算符时，你有效地使用表的键。在if语句的情况下，in运算符将检查键集合中的成员资格，并返回true或false值。该示例显示如何检查SMTPS是否不在ssl_services表的键集合中，如果条件成立，我们将键值对添加到表中。最后，该示例显示如何使用for语句来迭代表中当前的每个键。除了简单的例子，表可能变得非常复杂，因为表的键和值变得更复杂。表可以具有由多种数据类型组成的键，甚至包括一系列称为“元组”的元素。在Bro中使用复杂表格所获得的灵活性意味着编写脚本的人的高复杂性成本，但是由于Bro作为网络安全平台的强大性，有效性得到了提高。\ndata_struct_table_complex.broevent bro_init()    &#123;    local samurai_flicks: table[string, string, count, string] of string;    samurai_flicks[&quot;Kihachi Okamoto&quot;, &quot;Toho&quot;, 1968, &quot;Tatsuya Nakadai&quot;] = &quot;Kiru&quot;;    samurai_flicks[&quot;Hideo Gosha&quot;, &quot;Fuji&quot;, 1969, &quot;Tatsuya Nakadai&quot;] = &quot;Goyokin&quot;;    samurai_flicks[&quot;Masaki Kobayashi&quot;, &quot;Shochiku Eiga&quot;, 1962, &quot;Tatsuya Nakadai&quot; ] = &quot;Harakiri&quot;;    samurai_flicks[&quot;Yoji Yamada&quot;, &quot;Eisei Gekijo&quot;, 2002, &quot;Hiroyuki Sanada&quot; ] = &quot;Tasogare Seibei&quot;;    for ( [d, s, y, a] in samurai_flicks )    print fmt(&quot;%s was released in %d by %s studios, directed by %s and starring %s&quot;, samurai_flicks[d, s, y, a], y, s, d, a);    &#125;\n执行data_struct_table_complex.bro脚本：\n# bro -b data_struct_table_complex.broHarakiri was released in 1962 by Shochiku Eiga studios, directed by Masaki Kobayashi and starring Tatsuya NakadaiGoyokin was released in 1969 by Fuji studios, directed by Hideo Gosha and starring Tatsuya NakadaiTasogare Seibei was released in 2002 by Eisei Gekijo studios, directed by Yoji Yamada and starring Hiroyuki SanadaKiru was released in 1968 by Toho studios, directed by Kihachi Okamoto and starring Tatsuya Nakadai\n此脚本显示由两个字符串索引的字符串，一个计数和一个最后一个字符串的示例表。使用元组作为聚合键，顺序很重要，因为顺序的改变将导致新的键。在这里，我们使用表来跟踪导演，工作室，年份或发行版和一系列的的主演。重要的是要注意，在for语句的情况下，它是一个全类型或无类型的迭代。我们不能重复，例如，董事;我们必须以确切的格式作为键本身进行迭代。在这种情况下，我们需要围绕四个临时变量的方括号作为我们迭代的集合。虽然这是一个假设的例子，我们可以很容易地有包含IP地址（addr），端口（port）的键，甚至一个字符串作为反向计算主机名查找的结果。\nVectors如果你处于Bro的编程环境下，你可能会或不熟悉矢量数据类型，具体取决于你选择的语言。表面上，向量执行与具有无符号整数作为其索引的关联数组相同的功能。然而，它们比这更有效，并且允许有序访问。因此，任何时候，你需要顺序存储相同类型的数据，在Bro你应该选择vector。vector是对象的集合，所有对象都具有相同的数据类型，元素可以动态添加或删除。由于矢量对其元素使用连续存储，所以可以通过 zero-index 的数值偏移来访问 vector 的内容。Vector声明的格式遵循其他声明的模式，即SCOPE v：vector of T，其中v是 vector 的名称，T 是其成员的数据类型。例如，以下代码片段显示了两个局部范围 vector 的显式和隐式声明。脚本通过在末尾插入值来填充第一个向量;它通过在两个垂直管道之间放置vector名称来获得vector的当前长度，然后打印两个vector的内容及其当前长度。\ndata_struct_vector_declaration.broevent bro_init()    &#123;    local v1: vector of count;    local v2 = vector(1, 2, 3, 4);    v1[|v1|] = 1;    v1[|v1|] = 2;    v1[|v1|] = 3;    v1[|v1|] = 4;    print fmt(&quot;contents of v1: %s&quot;, v1);    print fmt(&quot;length of v1: %d&quot;, |v1|);    print fmt(&quot;contents of v2: %s&quot;, v2);    print fmt(&quot;length of v2: %d&quot;, |v2|);    &#125;\n执行data_struct_vector_declaration.bro脚本：\n# bro data_struct_vector_declaration.brocontents of v1: [1, 2, 3, 4]length of v1: 4contents of v2: [1, 2, 3, 4]length of v2: 4\n在很多情况下，vector中存储元素仅仅是用于对它们进行迭代。使用for关键字很容易迭代向量。下面的示例迭代一个IP地址的vector，对于每个IP地址，掩码以18位地址。 for关键字用于生成一个局部范围的变量 i，它将保存向量中当前元素的索引。使用i作为addr_vector的索引，我们可以使用addr_vector [i]访问vector中的当前项。\ndata_struct_vector_iter.broevent bro_init()    &#123;    local addr_vector: vector of addr = vector(1.2.3.4, 2.3.4.5, 3.4.5.6);    for (i in addr_vector)    print mask_addr(addr_vector[i], 18);    &#125;\n执行data_struct_vector_iter.bro脚本：\n# bro -b data_struct_vector_iter.bro1.2.0.0/182.3.0.0/183.4.0.0/18\nData Types Revisitedaddr地址或地址数据类型管理覆盖惊人的大量地面，同时保持简洁。 IPv4，IPv6甚至主机名常量都包含在addr数据类型中。虽然IPv4地址使用默认的点分四元格式，但IPv6地址使用RFC 2373定义的符号，加上整数地址的方括号。当你闯进一台主机时，Bro会处于用户的角度做一点手脚；主机名常量实际上是一组地址。 Bro将在它看到正在使用的主机名常量时发出DNS请求，并返回其元素是DNS请求答案的集合。例如，如果您要使用本地google &#x3D; www.google.com ;你最终会得到一个局部范围的set[addr]，其中的元素代表google的当前循环DNS条目集。初看起来，这看起来微不足道，但Bro的另一个例子，通过以实用的方式应用抽象，使得通用Bro脚本更方便一些。 （请注意，这些IP地址永远不会在Bro处理期间更新，因此通常此机制对于预期保持静态的地址最有用.）\nportBro中的传输层端口号以  &#x2F; &lt; protocol name &gt; 的格式表示，例如22 &#x2F; tcp或53 &#x2F; udp。 Bro支持TCP（&#x2F; tcp），UDP（&#x2F; udp），ICMP（&#x2F; icmp）和UNKNOWN（&#x2F;unknown）作为协议名称。虽然ICMP没有实际端口，Bro通过使用ICMP消息类型和ICMP消息代码分别作为源和目的端口支持ICMP“端口”的概念。端口可以​​使用 &#x3D;&#x3D; 或 !&#x3D; 运算符进行相等比较，甚至可以进行比较以进行排序。 Bro给予协议名称以下“order”：unknown&lt;tcp &lt;udp &lt;icmp。例如65535 &#x2F; tcp小于0 &#x2F; udp。\nsubnetBro对CIDR表示法子网具有完全支持作为基本数据类型。当您可以在脚本中以CIDR表示法提供相同的信息时，不需要将IP和子网掩码管理为两个单独的实体。以下示例使用Bro脚本来确定一系列IP地址是否位于使用20位子网掩码的一组子网内。\ndata_type_subnets.broevent bro_init()    &#123;    local subnets = vector(172.16.0.0/20, 172.16.16.0/20, 172.16.32.0/20, 172.16.48.0/20);    local addresses = vector(172.16.4.56, 172.16.47.254, 172.16.22.45, 172.16.1.1);    for ( a in addresses )    &#123;    for ( s in subnets )        &#123;        if ( addresses[a] in subnets[s] )            print fmt(&quot;%s belongs to subnet %s&quot;, addresses[a], subnets[s]);        &#125;    &#125;    &#125;\n因为这是一个不使用任何类型的网络分析的脚本，我们可以处理事件bro_init，它始终由Bro的核心在启动时生成。在示例脚本中，创建两个本地作用域向量以分别保存我们的子网和IP地址列表。然后，使用一组嵌套for循环，我们遍历每个子网和每个IP地址，并使用if语句使用in运算符来比较IP地址和子网。如果IP地址基于最长前缀匹配计算落入给定子网内，则in运算符返回true。例如，10.0.0.0&#x2F;8中的10.0.0.1将返回true，而192.168.1.0&#x2F;24中的192.168.2.1将返回false。当我们运行脚本时，我们得到输出列出它所属的IP地址和子网。\n# bro data_type_subnets.bro172.16.4.56 belongs to subnet 172.16.0.0/20172.16.47.254 belongs to subnet 172.16.32.0/20172.16.22.45 belongs to subnet 172.16.16.0/20172.16.1.1 belongs to subnet 172.16.0.0/20\ntime虽然当前没有支持的方法在Bro中添加时间常数，但是存在两个内置函数以利用时间数据类型。 network_time和current_time都返回一个时间数据类型，但它们各自根据不同的标准返回一个时间。 current_time函数返回由操作系统定义的所谓挂钟时间。但是，network_time返回从实时数据流或保存的数据包捕获中处理的最后一个数据包的时间戳。这两个函数以时代秒返回时间，这意味着必须使用strftime将输出转换为人类可读的输出。下面的脚本利用connection_established事件处理程序，在每次看到SYN &#x2F; ACK数据包响应SYN数据包作为TCP握手的一部分时生成文本。生成的文本采用时间戳的格式，并指示发起者和响应者是谁。我们使用％Y％M％d％H：％m：％S的strftime格式字符串产生一个通用的日期时间格式的时间戳。\ndata_type_time.broevent connection_established(c: connection)    &#123;    print fmt(&quot;%s:  New connection established from %s to %s\\n&quot;, strftime(&quot;%Y/%M/%d %H:%m:%S&quot;, network_time()), c$id$orig_h, c$id$resp_h);    &#125;\n当脚本执行时，我们得到一个输出，显示已建立的连接的细节。\n# bro -r wikipedia.trace data_type_time.bro2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.118\\x0a2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3\\x0a2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3\\x0a2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3\\x0a2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3\\x0a2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3\\x0a2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3\\x0a2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.2\\x0a2011/06/18 19:03:09:  New connection established from 141.142.220.235 to 173.192.163.128\\x0a\n\ninterval区间数据类型是Bro中的另一个区域，其中抽象的合理应用是完美的。作为数据类型，间隔表示由数字常数跟随以时间单位表示的相对时间。例如，2.2秒将是2.2秒，三十一天将由31天表示。 Bro支持分别表示微秒，毫秒，秒，分钟，小时和天的usec，msec，sec，min，hr或day。事实上，间隔数据类型允许其定义中出乎意料的变化量。在数字常数之间可以有一个空格，或者它们可以像时间端口一样被挤在一起。时间单位可以是单数或复数。所有这一切加在一起的事实，42小时和42小时是完全有效的，在逻辑上相当于在布罗。然而，要点是增加脚本的可读性和可维护性。间隔甚至可以否定，允许-10分钟表示“十分钟前”。Bro中的间隔可以对其执行数学运算，允许用户执行加法，减法，乘法，除法和比较运算。此外，当使用 - 运算符比较两个时间值时，Bro返回一个间隔。下面的脚本修改了上一节中启动的脚本，以包括与连接建立报告一起打印的时间增量值。\ndata_type_interval.bro# Store the time the previous connection was established.global last_connection_time: time;# boolean value to indicate whether we have seen a previous connection.global connection_seen: bool = F;event connection_established(c: connection)    &#123;    local net_time: time  = network_time();    print fmt(&quot;%s:  New connection established from %s to %s&quot;, strftime(&quot;%Y/%M/%d %H:%m:%S&quot;, net_time), c$id$orig_h, c$id$resp_h);    if ( connection_seen )    print fmt(&quot;     Time since last connection: %s&quot;, net_time - last_connection_time);    last_connection_time = net_time;    connection_seen = T;    &#125;\n这一次，当我们执行脚本时，我们在输出中看到一个额外的行，显示自上次完全建立的连接以来的时间增量。\n# bro -r wikipedia.trace data_type_interval.bro2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.1182011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3     Time since last connection: 132.0 msecs 97.0 usecs2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3     Time since last connection: 177.0 usecs2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3     Time since last connection: 2.0 msecs 177.0 usecs2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3     Time since last connection: 33.0 msecs 898.0 usecs2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3     Time since last connection: 35.0 usecs2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.3     Time since last connection: 2.0 msecs 532.0 usecs2011/06/18 19:03:08:  New connection established from 141.142.220.118 to 208.80.152.2     Time since last connection: 7.0 msecs 866.0 usecs2011/06/18 19:03:09:  New connection established from 141.142.220.235 to 173.192.163.128     Time since last connection: 817.0 msecs 703.0 usecs\nPatternBro支持使用正则表达式进行快速文本搜索操作，甚至为正则表达式中使用的模式声明本机数据类型。通过在正斜杠字符中包含文本来创建模式常数。 Bro支持与Flex词法分析器语法非常相似的语法。在Bro中最常见的模式使用可能是使用in运算符的嵌入式匹配。嵌入式匹配遵循严格的格式，要求正则表达式或模式常量在in运算符的左侧，要求测试的字符串在右侧。\ndata_type_pattern_01.broevent bro_init()&#123;local test_string = &quot;The quick brown fox jumps over the lazy dog.&quot;;local test_pattern = /quick|lazy/;if ( test_pattern in test_string )    &#123;    local results = split(test_string, test_pattern);    print results[1];    print results[2];    print results[3];    &#125;&#125;\n在上面的示例中，声明了两个局部变量来保存我们的示例句和正则表达式。在这种情况下，如果字符串包含单词quick或单词lazy，我们的正则表达式将返回true。脚本中的if语句使用嵌入式匹配和in运算符来检查字符串中是否存在模式。如果语句解析为true，则调用split以将字符串拆分为单独的片段。 Split使用字符串和模式作为其参数，并返回由计数索引的字符串表。表的每个元素将是与模式匹配的前后的段，但不包括实际匹配。在这种情况下，我们的模式匹配两次，并产生一个包含三个条目的表。脚本中的打印语句将按顺序打印表的内容。\n# bro data_type_pattern_01.broThe brown fox jumps over the dog.模式也可以用于通过==和！=运算符分别使用等式和不等式运算符来比较字符串。但是，当以这种方式使用时，字符串必须完全匹配才能解析为true。例如，下面的脚本使用两个三元条件语句来说明==运算符与模式的使用。基于模式和字符串之间的比较结果改变输出。data_type_pattern_02.broevent bro_init()&#123;local test_string = &quot;equality&quot;;local test_pattern = /equal/;print fmt(&quot;%s and %s %s equal&quot;, test_string, test_pattern, test_pattern == test_string ? &quot;are&quot; : &quot;are not&quot;);test_pattern = /equality/;print fmt(&quot;%s and %s %s equal&quot;, test_string, test_pattern, test_pattern == test_string ? &quot;are&quot; : &quot;are not&quot;);&#125;\n执行bro脚本：\n# bro data_type_pattern_02.broequality and /^?(equal)$?/ are not equalequality and /^?(equality)$?/ are equal\nRecord Data Type在Bro支持各种数据类型和数据结构的情况下，一个明显的扩展是包括创建由原子类型和其他数据结构组成的自定义数据类型的能力。为了实现这一点，Bro引入了记录类型和类型关键字。与使用typedef和struct关键字在C中定义新数据结构类似，Bro允许您将新数据类型拼凑在一起以适应您的情况。当与type关键字组合时，record可以生成复合类型。事实上，我们已经在前面的章节中遇到过一个复杂的记录数据类型的例子，连接记录传递给许多事件。另一个，Conn :: Info，它对应于记录到conn.log的字段，由下面的摘录显示。\ndata_type_record.bromodule Conn;export &#123;    ## The record type which contains column fields of the connection log.    type Info: record &#123;        ts:           time            &amp;log;        uid:          string          &amp;log;        id:           conn_id         &amp;log;        proto:        transport_proto &amp;log;        service:      string          &amp;log &amp;optional;        duration:     interval        &amp;log &amp;optional;        orig_bytes:   count           &amp;log &amp;optional;        resp_bytes:   count           &amp;log &amp;optional;        conn_state:   string          &amp;log &amp;optional;        local_orig:   bool            &amp;log &amp;optional;        local_resp:   bool            &amp;log &amp;optional;        missed_bytes: count           &amp;log &amp;default=0;        history:      string          &amp;log &amp;optional;        orig_pkts:     count      &amp;log &amp;optional;        orig_ip_bytes: count      &amp;log &amp;optional;        resp_pkts:     count      &amp;log &amp;optional;        resp_ip_bytes: count      &amp;log &amp;optional;        tunnel_parents: set[string] &amp;log;    &#125;;&#125;\n看一下定义的结构，一个新的数据类型集合被定义为一个名为Info的类型。因为这种类型定义在导出块的范围内，所以定义的是事实上的Conn :: Info。Bro中记录类型的声明的格式化包括要定义的类型的描述性名称和组成记录的单独字段。组成新记录的单个字段在类型或数量上不受限制，只要每个字段的名称是唯一的。\ndata_struct_record_01.brotype Service: record &#123;    name: string;    ports: set[port];    rfc: count;&#125;;function print_service(serv: Service)    &#123;    print fmt(&quot;Service: %s(RFC%d)&quot;,serv$name, serv$rfc);    for ( p in serv$ports )    print fmt(&quot;  port: %s&quot;, p);    &#125;event bro_init()    &#123;    local dns: Service = [$name=&quot;dns&quot;, $ports=set(53/udp, 53/tcp), $rfc=1035];    local http: Service = [$name=&quot;http&quot;, $ports=set(80/tcp, 8080/tcp), $rfc=2616];    print_service(dns);    print_service(http);    &#125;\n执行上述脚本：\n# bro data_struct_record_01.broService: dns(RFC1035)  port: 53/udp  port: 53/tcpService: http(RFC2616)  port: 8080/tcp  port: 80/tcp\n上面的示例显示了一个简单的类型定义，其中包括字符串，一组端口和用于定义服务类型的计数。还包括一个以格式化方式打印记录的每个字段的函数和一个显示处理记录的某些功能的bro_init事件处理程序。 DNS和HTTP服务的定义在传递给print_service函数之前都使用方括号进行内联。 print_service函数使用$ dereference运算符来访问新定义的服务记录类型中的字段。正如你在Conn :: Info记录的定义中看到的，其他记录甚至有效作为另一个记录中的字段。我们可以扩展上面的示例，以包含另一个包含服务记录的记录。\ndata_struct_record_02.brotype Service: record &#123;    name: string;    ports: set[port];    rfc: count;&#125;;type System: record &#123;    name: string;    services: set[Service]; &#125;;function print_service(serv: Service)    &#123;    print fmt(&quot;  Service: %s(RFC%d)&quot;,serv$name, serv$rfc);    for ( p in serv$ports )    print fmt(&quot;    port: %s&quot;, p);    &#125;function print_system(sys: System)    &#123;    print fmt(&quot;System: %s&quot;, sys$name);    for ( s in sys$services )    print_service(s);    &#125;event bro_init()    &#123;    local server01: System;    server01$name = &quot;morlock&quot;;    add server01$services[[ $name=&quot;dns&quot;, $ports=set(53/udp, 53/tcp), $rfc=1035]];    add server01$services[[ $name=&quot;http&quot;, $ports=set(80/tcp, 8080/tcp), $rfc=2616]];    print_system(server01);    # local dns: Service = [ $name=&quot;dns&quot;, $ports=set(53/udp, 53/tcp), $rfc=1035];    # local http: Service = [ $name=&quot;http&quot;, $ports=set(80/tcp, 8080/tcp), $rfc=2616];    # print_service(dns);    # print_service(http);    &#125;\n执行上述脚本：\n# bro data_struct_record_02.broSystem: morlock  Service: http(RFC2616)    port: 8080/tcp    port: 80/tcp  Service: dns(RFC1035)    port: 53/udp    port: 53/tcp\n上面的示例包括其中字段用作集合的数据类型的第二记录类型。记录可以重复嵌套在其他记录中，它们的字段可通过$ dereference运算符的重复链来实现。还常见的一种类型是用于简单地将数据结构别名为更具描述性的名称。下面的示例显示了一个来自Bro自己的类型定义文件的示例。\ninit-bare.brotype string_array: table[count] of string;type string_set: set[string];type addr_set: set[addr];\n上面的三行将一种类型的数据结构别名为描述性名称。在功能上，操作是相同的，然而，上述每种类型的命名使得它们的功能可立即识别。这是Bro脚本中的另一个地方，其中考虑可以导致代码的更好的可读性，从而在将来更容易维护。\nCustom Logging通过对Bro中的数据类型和数据结构的正确理解，探索各种可用的框架是一个更有价值的工作。大多数用户可能使用中最多交互的框架是日志框架。以这样一种方式设计，以便抽象创建文件和将组织好的有序的数据附加到其中的大部分过程，记录框架利用一些可能不熟悉的命名。具体来说，日志流，过滤器和写入器只是对管理高速传入日志所需的进程的抽象，同时保持完全可操作性。如果你在一个拥有大量连接的环境中使用Bro，你会发现日志的生成速度非常快，处理大量数据并将其写入磁盘的能力都归功于Logging Framework的设计。基于Bro的脚本中的决策过程，将数据写入日志流。日志流对应于由组成其字段的 name&#x2F;value 对集合定义的单个日志。然后，可以使用记录筛选器对数据进行筛选，修改或重定向，这些筛选器默认设置为记录一切。过滤器可用于将日志文件拆分为子集或将该信息复制到另一个输出。数据的最终输出由写入程序定义。 Bro的默认编写器是简单的制表符分隔的ASCII文件，但Bro还支持DataSeries和Elasticsearch输出以及当前正在开发的其他编写器。虽然这些新的术语和想法可能给人的印象是日志框架很难使用，但实际的学习曲线实际上并不是非常陡峭。日志框架中内置的抽象使得绝大多数脚本都很基础。实际上，写入日志文件与定义数据格式一样简单，让Bro知道你希望创建一个新日志，然后调用Log :: write方法来输出日志记录。日志框架是Bro中的一个区域，你看到它使用的越多，你自己使用的越多，代码的样板部分将越多使用。因此，让我们通过一个简单的例子，简单地记录数字1到10和他们相应的阶乘到默认ASCII日志记录器。最好一次性处理问题，在尝试深入到日志框架之前，使用print和fmt模拟所需的输出。\nframework_logging_factorial_01.bromodule Factor;function factorial(n: count): count    &#123;    if ( n == 0 )    return 1;    else    return ( n * factorial(n - 1) );    &#125;event bro_init()    &#123;    local numbers: vector of count = vector(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);    for ( n in numbers )    print fmt(&quot;%d&quot;, factorial(numbers[n]));    &#125;\n执行上述脚本：\n# bro framework_logging_factorial_01.bro126241207205040403203628803628800\n此脚本定义了一个阶乘函数，递归计算作为函数参数传递的无符号整数的阶乘。使用print和fmt，我们可以确保Bro可以正确地执行这些计算，并自己得到答案的想法。脚本的输出与我们期望的一致，现在是时候集成日志框架。\nframework_logging_factorial_02.bromodule Factor;export &#123;    # Append the value LOG to the Log::ID enumerable.    redef enum Log::ID += &#123; LOG &#125;;    # Define a new type called Factor::Info.    type Info: record &#123;    num:           count &amp;log;    factorial_num: count &amp;log;    &#125;;    &#125;function factorial(n: count): count    &#123;    if ( n == 0 )    return 1;    else    return ( n * factorial(n - 1) );    &#125;event bro_init()    &#123;    # Create the logging stream.    Log::create_stream(LOG, [$columns=Info, $path=&quot;factor&quot;]);    &#125;event bro_done()    &#123;    local numbers: vector of count = vector(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);        for ( n in numbers )    Log::write( Factor::LOG, [$num=numbers[n],                              $factorial_num=factorial(numbers[n])]);    &#125;\n如上所述，我们必须执行几个步骤，然后才能发出Log :: write方法并生成日志文件。由于我们在命名空间中工作，并向外部实体通知命名空间内部的工作和数据，因此我们使用导出块。首先，我们需要告知Bro，我们将通过向Log :: ID枚举中添加一个值来添加另一个日志流。在这个脚本中，我们将Log值附加到Log :: ID枚举中，但是由于这是一个导出块，附加到Log :: ID的值实际上是Factor :: Log。接下来，我们需要定义构成我们日志数据并指定其格式的名称和值对。此脚本定义了一个名为Info（实际上，Factor :: Info）的新记录数据类型，具有两个字段，两个无符号整数。 Factor :: Log记录类型中的每个字段都包含＆log属性，表示当调用Log :: write时，这些字段应传递到日志框架。如果没有＆log属性的任何名称值对，这些字段将在日志记录期间被忽略，但在变量的生命周期中仍然可用。下一步是使用Log :: create_stream创建日志流，它使用Log :: ID和记录作为其参数。在这个例子中，我们调用Log :: create_stream方法，并传递Factor :: LOG和Factor :: Info记录作为参数。从这里开始，如果我们使用正确的Log :: ID和正确格式化的Factor :: Info记录发出Log :: write命令，将生成一个日志条目。现在，如果我们运行此脚本，而不是生成到stdout的日志记录信息，则不会创建输出。相反，输出都在factor.log，正确格式化和组织。\n# bro framework_logging_factorial_02.bro如下：#separator \\x09#set_separator    ,#empty_field      (empty)#unset_field      -#path     factor#open     2016-11-18-19-00-13#fields   num     factorial_num#types    count   count1 12 23 64 245 1206 7207 50408 403209 36288010        3628800#close    2016-11-18-19-00-13\n虽然前面的例子是一个简单的例子，它用于演示为了生成日志需要到位的小块脚本代码。例如，通常在bro_init中调用Log :: create_stream，而在实例中，确定何时调用Log :: write可能在事件处理程序中完成，在这种情况下我们使用bro_done。如果您已经花费了部署Bro的时间，那么您可能有机会查看，搜索或处理由记录框架生成的日志。 Bro的默认安装的日志输出是可以说的，但是，有时候，Logging Framework默认情况下不是理想的方式。这可以包括从每次调用Log :: write时需要记录更多或更少的数据，或者甚至需要基于任意逻辑分割日志文件。在后一种情况下，过滤器与日志框架一起使用。筛选器向Bro的脚本库授予一定级别的自定义，允许脚本作者在日志中包含或排除字段，甚至更改放置日志的文件的路径。每个流创建时，都会提供一个默认过滤器，不出所料，默认为。当使用默认过滤器时，每个具有＆log属性的键值对都将写入单个文件。对于我们一直使用的例子，让我们扩展它，以便写一个因子为5的因子到备用文件，同时将剩余的日志写入factor.log。\nframework_logging_factorial_03.broevent bro_init()    &#123;    Log::create_stream(LOG, [$columns=Info, $path=&quot;factor&quot;]);    local filter: Log::Filter = [$name=&quot;split-mod5s&quot;, $path_func=mod5];    Log::add_filter(Factor::LOG, filter);    Log::remove_filter(Factor::LOG, &quot;default&quot;);    &#125;\n要动态更改流写入其日志的文件，过滤器可以指定一个函数，该函数返回一个字符串以用作当前调用Log :: write的文件名。此函数的定义必须以一个称为id的Log :: ID作为其参数，一个名为path的字符串，以及称为rec的日志的相应记录类型。您可以看到本示例中使用的mod5的定义符合该要求。如果阶乘可以被5整除，函数简单地返回factor-mod5，否则返回因子non5。在额外的bro_init事件处理程序中，我们定义一个局部范围的Log :: Filter，并为它分配一个定义名称和path_func字段的记录。然后我们调用Log :: add_filter将过滤器添加到Factor :: LOG Log :: ID并调用Log :: remove_filter来删除Factor :: LOG的默认过滤器。如果我们没有删除默认过滤器，我们最终会得到三个日志文件：factor-mod5.log，所有的因子都是因子5，factor-non5.log的因子不是5的因子，和factor.log，其中包括所有阶乘。\n# bro framework_logging_factorial_03.bro如下：#separator \\x09#set_separator    ,#empty_field      (empty)#unset_field      -#path     factor-mod5#open     2016-11-18-19-00-14#fields   num     factorial_num#types    count   count5 1206 7207 50408 403209 36288010        3628800#close    2016-11-18-19-00-14\nBro生成易于定制和可扩展的日志，这些日志保持容易解析的能力是Bro获得了大量重视的重要原因。事实上，有时很难想到Bro不会记录的事情，因此分析师和系统架构师通常会优先考虑日志框架，以便能够根据发送的数据执行自定义操作到记录框。为此，Bro中的每个默认日志流都生成一个自定义事件，任何希望对发送到流的数据执行操作的人都可以处理该事件。按照惯例，这些事件通常采用log_x格式，其中x是日志记录流的名称;因此由HTTP解析器发送到日志记录框架的每个日志引发的事件将是log_http。事实上，我们已经看到一个脚本处理log_http事件，当我们中断了如何检测MHR.bro脚本工作。在该示例中，当每个日志条目发送到日志记录框架时，在log_http事件中进行后处理。代替使用外部脚本来解析http.log文件并对该条目进行后处理，后处理可以在Bro中实时完成。告诉Bro在自己的Logging流中引发一个事件就像导出该事件名一样简单，然后在调用Log :: create_stream时添加该事件。回到我们记录整数阶乘的简单例子，我们向导出块添加log_factor并定义要传递给它的值，在这种情况下是Factor :: Info记录。然后，我们在调用Log :: create_stream时将log_factor函数列为$ ev字段\nframework_logging_factorial_04.bromodule Factor;export &#123;    redef enum Log::ID += &#123; LOG &#125;;    type Info: record &#123;    num:           count &amp;log;    factorial_num: count &amp;log;    &#125;;    global log_factor: event(rec: Info);    &#125;function factorial(n: count): count    &#123;    if ( n == 0 )    return 1;    else    return (n * factorial(n - 1));    &#125;event bro_init()    &#123;    Log::create_stream(LOG, [$columns=Info, $ev=log_factor, $path=&quot;factor&quot;]);    &#125;event bro_done()    &#123;    local numbers: vector of count = vector(1, 2, 3, 4, 5, 6, 7, 8, 9, 10);        for ( n in numbers )    Log::write( Factor::LOG, [$num=numbers[n],                              $factorial_num=factorial(numbers[n])]);    &#125;function mod5(id: Log::ID, path: string, rec: Factor::Info) : string        &#123;    if ( rec$factorial_num % 5 == 0 )    return &quot;factor-mod5&quot;;    else    return &quot;factor-non5&quot;;    &#125;event bro_init()    &#123;    local filter: Log::Filter = [$name=&quot;split-mod5s&quot;, $path_func=mod5];    Log::add_filter(Factor::LOG, filter);    Log::remove_filter(Factor::LOG, &quot;default&quot;);    &#125;\nRaising Notices尽管Bro的日志框架提供了一种简单和系统的生成日志的方法，但仍然需要指示何时检测到特定行为，以及允许该检测得到某人注意的方法。为此，“通知框架”已到位，允许脚本作者通过编码方式提出通知，以及运营商可以选择接收通知的系统。 Bro坚持这样的理念，即由个体操作者指示他们感兴趣的行为，并且这样的Bro具有大量的策略脚本，其检测可能感兴趣的行为，但是它不假设为猜测行为是“可行动的”。实际上，Bro致力于分离检测行为和报告责任。使用通知框架，为检测到的任何行为提出通知是很容易的。要在Bro提出通知，您只需要向Bro表示您通过导出提供一个特定的Notice :: Type，然后致电NOTICE为其提供适当的Notice :: Info记录。通常，对NOTICE的调用只包括Notice :: Type和一个简明的消息。然而，当提醒通知时，有显着更多的选项，如在Notice :: Info的定义中看到的。 Notice :: Info中唯一的属性为必填字段的字段是note字段。然而，良好的方式总是重要的，并且包括$ msg中的简明消息，并且在必要时，$ conn中的连接记录的内容连同Notice ::类型倾向于包括要考虑的通知所需的最少信息有用。如果提供了$ conn变量，Notice Framework将自动填充$ id和$ src字段。通常包括的其他字段，$ identifier和$ suppress_for是围绕“通知框架”的自动抑制功能构建的，我们将在稍后介绍。其中一个默认策略脚本在启动式检测到SSH登录时发出通知，并且原始主机名将引起怀疑。有效地，脚本尝试定义一个主机列表，您不想从中查看源自SSH流量的DNS流量，例如DNS服务器，邮件服务器等。为了实现这一点，脚本遵循通过检测行为分离检测和报告并提出通知。该通知是否被执行取决于本地通知政策，但脚本尝试提供尽可能多的信息，同时保持简洁。\ninteresting-hostnames.bro##! This script will generate a notice if an apparent SSH login originates ##! or heads to a host with a reverse hostname that looks suspicious.  By ##! default, the regular expression to match &quot;interesting&quot; hostnames includes ##! names that are typically used for infrastructure hosts like nameservers, ##! mail servers, web servers and ftp servers.@load base/frameworks/noticemodule SSH;export &#123;    redef enum Notice::Type += &#123;        ## Generated if a login originates or responds with a host where        ## the reverse hostname lookup resolves to a name matched by the        ## :bro:id:`SSH::interesting_hostnames` regular expression.        Interesting_Hostname_Login,    &#125;;    ## Strange/bad host names to see successful SSH logins from or to.    const interesting_hostnames =            /^d?ns[0-9]*\\./ |            /^smtp[0-9]*\\./ |            /^mail[0-9]*\\./ |            /^pop[0-9]*\\./  |            /^imap[0-9]*\\./ |            /^www[0-9]*\\./  |            /^ftp[0-9]*\\./  &amp;redef;&#125;function check_ssh_hostname(id: conn_id, uid: string, host: addr)    &#123;    when ( local hostname = lookup_addr(host) )        &#123;        if ( interesting_hostnames in hostname )            &#123;            NOTICE([$note=Interesting_Hostname_Login,                    $msg=fmt(&quot;Possible SSH login involving a %s %s with an interesting hostname.&quot;,                             Site::is_local_addr(host) ? &quot;local&quot; : &quot;remote&quot;,                             host == id$orig_h ? &quot;client&quot; : &quot;server&quot;),                    $sub=hostname, $id=id, $uid=uid]);            &#125;        &#125;    &#125;event ssh_auth_successful(c: connection, auth_method_none: bool)    &#123;    for ( host in set(c$id$orig_h, c$id$resp_h) )        &#123;        check_ssh_hostname(c$id, c$uid, host);        &#125;    &#125;\n虽然大部分脚本与实际检测有关，但“通知框架”特有的部分本身实际上是相当有趣的。脚本的导出块将值SSH :: Interesting_Hostname_Login添加到可枚举常量Notice :: Type中，以向Bro核指示正在定义新类型的通知。脚本然后调用NOTICE并定义Notice :: Info记录的$ note，$ msg，$ sub，id和$ uid字段。 （更常见的是，将设置$ conn，但是这个脚本为了性能原因避免使用在when语句内的连接记录）。有两个三元if语句修改$ msg文本取决于主机是否是本地地址以及它是客户端还是服务器。这种使用fmt和三元运算符是一种简单的方式，可以为生成的通知提供可读性，而无需分支（如果每个语句都提出特定通知）。通知的选择加入系统通过写Notice ::策略钩子来管理。 Notice :: policy钩子接受一个Notice :: Info记录作为其参数，它将保存你的脚本在调用NOTICE中提供的相同信息。通过访问Notice :: Info记录获取特定通知，您可以在挂钩的正文中包含诸如in语句的逻辑，以更改系统上处理通知的策略。在Bro中，钩子类似于函数和事件处理器的混合：类似函数，对它们的调用是同步的（即运行到完成和返回）;但像事件，他们可以有多个机构，都将执行。为了定义通知策略，您定义一个钩子，Bro将负责传递Notice :: Info记录。最简单的Notice :: policy钩子只是检查在通知挂钩中的Notice :: Info记录中的$ note的值，并根据答案执行一个动作。下面的钩子为在策略&#x2F; protocols &#x2F; ssh &#x2F; interesting-hostnames.bro脚本中引发的SSH :: Interesting_Hostname_Login通知添加Notice :: ACTION_EMAIL操作。\nframework_notice_hook_01.bro@load policy/protocols/ssh/interesting-hostnames.brohook Notice::policy(n: Notice::Info)  &#123;  if ( n$note == SSH::Interesting_Hostname_Login )      add n$actions[Notice::ACTION_EMAIL];  &#125;\n在上面的例子中，我们添加了Notice :: ACTION_EMAIL到n $ actions集。在Notice框架脚本中定义的此集合只能包含Notice :: Action类型的条目，它本身是可枚举的，它定义下表中显示的值及其相应的含义。 Notice :: ACTION_LOG操作将通知写入Notice :: LOG记录流，在默认配置中，它将每个通知写入notice.log文件，并且不进行进一步操作。 Notice :: ACTION_EMAIL操作将发送电子邮件到Notice :: mail_dest变量中定义的地址或地址，通知的详细信息将作为电子邮件的正文。最后一个操作，Notice :: ACTION_ALARM发送通知到Notice :: ALARM_LOG日志流，然后每小时旋转它的内容，并且其内容以可读ASCII以电子邮件发送到Notice :: mail_dest中的地址。\nACTION_NONE\tTake no actionACTION_LOG\tSend the notice to the Notice::LOG logging stream.ACTION_EMAIL\tSend an email with the notice in the body.ACTION_ALARM\tSend the notice to the Notice::Alarm_LOG stream.\n虽然Notice :: ACTION_EMAIL操作等操作具有快速警报和响应的吸引力，但是使用它的一个警告是确保配置了此操作的通知也具有抑制。抑制是一种手段，通过它们，如果脚本的作者已经设置了标识符，则在最初引发之后可以忽略通知。标识符是从连接相对于Bro所观察到的行为收集的唯一的信息字符串。\nexpiring-certs.broNOTICE([$note=Certificate_Expires_Soon,            $msg=fmt(&quot;Certificate %s is going to expire at %T&quot;, cert$subject, cert$not_valid_after),            $conn=c, $suppress_for=1day,            $identifier=cat(c$id$resp_h, c$id$resp_p, hash),            $fuid=fuid]);\n\n在策略&#x2F; protocols &#x2F; ssl &#x2F; expiring-certs.bro脚本中，该脚本标识何时SSL证书设置为过期，并在超过预定义阈值时引发通知，对上述NOTICE的调用还通过连接响应者IP设置$标识符条目，端口和证书的散列。响应者IP，端口和证书哈希的选择完全适合于适当的标识符，因为它创建了可以与其匹配的抑制的唯一标识符。如果我们取出用于标识符的任何实体，例如证书哈希，我们可能将我们的抑制设置得过于宽泛，导致分析人员错过了应该提出的通知。根据标识符的可用数据，设置$ suppress_for变量也很有用。 expiring-certs.bro脚本将$ suppress_for设置为1天，告知通知框架在第一个通知提出后24小时禁止通知。一旦该时间限制过去，可以提出另一个通知，其将再次设置1天抑制时间。抑制在特定时间量具有超越简单地不填写分析师的电子邮件收件箱的好处;及时和简明地保持通知警报有助于避免分析人员可能看到通知并且由于过度暴露而忽略它的情况。$ suppress_for变量也可以在Notice :: policy钩子中改变，允许部署更好地适合运行它的环境。使用expiring-certs.bro的示例，我们可以为SSL :: Certificate_Expires_Soon写一个Notice :: policy钩子，以将$ suppress_for变量配置为更短的时间。\nframework_notice_hook_suppression_01.bro@load policy/protocols/ssl/expiring-certs.brohook Notice::policy(n: Notice::Info)    &#123;   if ( n$note == SSL::Certificate_Expires_Soon )       n$suppress_for = 12hrs;   &#125;\n虽然Notice :: policy钩子允许您为部署构建自定义的基于谓词的策略，但是有一定次数，您不需要钩子允许的完全表达。简而言之，将有通知政策考虑，其中可以基于Notice :: Type单独做出广泛决定。为了促进这些类型的决策，“通知框架”支持“通知政策”快捷方式。这些快捷方式通过一组数据结构的方法来实现，这些数据结构将特定的，预定义的细节和动作映射到通知的有效名称。主要实现为Notice :: Type的枚举集或表，Notice Policy快捷方式可以作为一个简单的指令放置在local.bro文件中作为简明易读的配置。由于这些变量都是常量，因此需要提到这些变量都是在Bro完全启动并运行并且不是动态设置之前在解析时设置的。\nName\tDescription\tData TypeNotice::ignored_types\tIgnore the Notice::Type entirely\tset[Notice::Type]Notice::emailed_types\tSet Notice::ACTION_EMAIL to this Notice::Type\tset[Notice::Type]Notice::alarmed_types\tSet Notice::ACTION_ALARM to this Notice::Type\tset[Notice::Type]Notice::not_suppressed_types\tRemove suppression from this Notice::Type\tset[Notice::Type]Notice::type_suppression_intervals\tAlter the $suppress_for value for this Notice::Type\ttable[Notice::Type] of interval\n上表详细说明了五个Notice Policy快捷方式，它们的含义和用于实现它们的数据类型。除了Notice :: type_suppression_intervals之外，设置数据类型用于保存应该应用快捷方式的通知的Notice :: Type。前三个快捷键是相当自解释的，对集合中的Notice :: Type元素应用动作，而后两个快捷键更改应用于通知的抑制的细节。快捷方式Notice :: not_suppressed_types可用于从通知中删除配置的抑制，Notice :: type_suppression_intervals可用于更改由$ suppress_for在调用NOTICE中定义的抑制间隔。\nframework_notice_shortcuts_01.bro@load policy/protocols/ssh/interesting-hostnames.bro@load base/protocols/ssh/redef Notice::emailed_types += &#123;    SSH::Interesting_Hostname_Login&#125;;\n上面的Notice Policy快捷方式将Notice :: Type of SSH :: Interesting_Hostname_Login添加到Notice :: emailed_types集，而下面的快捷方式更改了那些通知被抑制的时间长度。\nframework_notice_shortcuts_02.bro@load policy/protocols/ssh/interesting-hostnames.bro@load base/protocols/ssh/redef Notice::type_suppression_intervals += &#123;    [SSH::Interesting_Hostname_Login] = 1day,&#125;;\n","categories":["网络安全"],"tags":["网络安全"]},{"title":"druid 源码分析之 filter-chain 机制","url":"/2019/04/16/druid-filter-chain/","content":"\nDruid，一个为监控而生的高性能数据库连接池，最近开始拜读温少的druid代码。接下来我将通过一系列文章记录阅读源码过程中的一些个人见解。本片文章讲述为druid带来强大扩展性的 filter-chain 模式。\n\nDruid的filter-chain模式相关的接口和类包括：Filter、FilterAdapter、FilterEventAdapter、FilterChain、FilterChainImpl、FilterManger以及相关具体的扩展实现 StatFilter、LogFilter 相关类。\nFilter的相关类及层级关系如下:\n\nFilter\nFilterAdapter\nFilterEventAdapter\nStatFilter\nLoggerFilter\n\n\n\n\n\n\nFilterChain\nFilterChainImpl\n\n\nFilterManager\n\n1、Filter具体功能实例在DataSource初始化时创建一个Filter链List，且是无状态或共享状态的。供由该DataSource派生的Connection、Statement、PreparedStatement等功能类共用；\n2、每一个功能类的实例都至少持有一个FilterChainImpl实例，FilterChainImpl相当于一个visitor遍历List。相当于一个Filter的连接器；\n3、FilterManager为Filter相关实现类加载器，通过配置文件初始化Filter；\n4、filters与autoFilters。\n相关类的具体职责如下：Filter接口：定义了过滤器需要关注的事件，以及可以处理的事件；FilterChain接口：定义过滤器关注的事件，与Filter职责类似；并串联Filter实例，并执行最终方法；FilterChainImpl类：FilterChian接口的具体实现；FilterAdapter抽象类：定义了基本的Filter接口默认实现；FilterEventAdapter抽象类：在FilterAdapter类的基础上，对关注的事件分为doBefore、do、doAfter相关操作；FilterManger类：使Filter具体实现可通过SPI方式加载；\nStatFilter、Slf4jLogFilter类：实现了doBefore,doAfter,这样的话，配置了这两个filter的类就可以做一些切面的事情了。\n下面通过Slf4jLogFilter类的加载及\n每个执行包装类实例中都包含一个FilterChainImpl实例，通过createChain()创建，通过recycleFilterChain(chain)进行回收再用。\n调用 setFilter() 时，Filter的加载过程：\n\n判断字符串是否以 ! 开头，如以 ! 开头，则清空之前加载的Filter链，再加载新的Filter实例；\n通过 FilterManager.loadFilter(List, String) 加载Filter实例到Filter链中；\n\n  2.1 FilterManager 类的静态代码块通过 SPI 方式先后通过 SystemClassLoader、FilterManager.class.getClassLoader、ThreadContextClassLoader、FilterManager.class.getClassLoader 加载4次 META-INF&#x2F;druid-filter.properties 获取Filter别名与类路径的Map映射；  2.2 根据用户的 setFilter() 实例化对应的Filter实例并加到List中。\n&lt;property name=&quot;validationQuery&quot; value=&quot;SELECT &#x27;x&#x27;&quot; /&gt;&lt;property name=&quot;testWhileIdle&quot; value=&quot;true&quot; /&gt;&lt;property name=&quot;testOnBorrow&quot; value=&quot;false&quot; /&gt;&lt;property name=&quot;testOnReturn&quot; value=&quot;false&quot; /&gt;","categories":["源码分析"]},{"title":"Git工作流的分支管理的解决方案","url":"/2018/03/30/git-flow/","content":"\n本文为基于git-flow工作流机制摸索实践出的一套工作流管理流程。\n\n1、分支名称及用途\nMaster：初始分支及生产对应分支，配置库创建之初最先创建的分支。上线后，该分支代码版本始终与生产环境保持一致，保证生产环境代码可追溯；\nHotFix：临时分支，当生产发现紧急问题，基于master分支拉取hotFix分支进行紧急问题修复；\nDevelop：长期的主开发基线，在该分支进行日常的开发及缺陷修复；\ndailyFix：日常版本发布基线，用于同步develop某一时点的版本，并基于该版本进行FAT、UAT发布验证；\nfeature&#x2F;dev1：某一时段的新需求开发分支，对应新需求验证环境；\nfeature&#x2F;dev2：某一时段的新需求开发分支，对应新需求验证环境；\nfeature&#x2F;dev…\n\n\n\n2、分支生命周期及对应环境\nMaster\n\nMaster始终存在，项目上线后总是与生产环境相对应。所有人员不能直接在master分支提交代码，只能从hotFix、dailyFix分支合并过来。合并时机为hotFix或dailyFix分支上线后的1-2天内。\n\nhotFix\n\n只有当生产上出现紧急问题，需紧急修复时，基于Master拉取hotFix针对生产的问题进行修复，并在准生产环境进行验证，验证完成后发布到生产环境，发布完成后hotFix分支同步到master和develop分支。同步到master保证master分支对应生产环境代码版本，同步到develop将修复的缺陷在FAT、UAT进行后置验证并进入到以后的版本中。\n\nDevelop\n\n初始从master分支产生，一致存在，作为主开发基线，日常版本的FAT、UAT版本都从这里从某一个时点发起向dailyFix分支合并。\n\ndailyFix\n\n日常FAT、UAT版本分支，每次版本分布时，需从develop分支将最新代码同步至dailyFix分支后，编译打包部署到FAT，待FAT验证通过，再进行编译打包到部署到UAT环境。\n\nfeature&#x2F;dev1、feature&#x2F;dev2、feature&#x2F;dev3……\n\n周期性新需求交付开发分支，当有新需求是，基于develop分支创建feature&#x2F;devx分支。该分支也有自己的日常新需求验证环境，新需求开发完成，会将该分支代码合并回主开发分支develop上，经FAT、UAT测试完成后实现交付。\n注意：该分支根据功能、业务模块、交付批次创建多个分支，粒度划分尽量合理：尽量避免不同交付批次的新需求在一个feature分支上，造成该分支生命周期过长；尽量避免交付时一个分支中包含本次交付和下次交付的需求，造成人工进行分支合并（从中挑选本次交付的需求，人工合并到develop分支上，产生大量人工工作量）。\n3、分支合并策略及时机\nDevelop –&gt; dailyFix\n\n日常FAT、UAT版本发布分支合并。合并时机：每次dailyFix发送测版本前进行合并；\n\ndailyFix –&gt; master\n\n阶段发布生产版本进行的分支合并。合并时机：每次dailyFix对应版本送测完成并上生产后1-2天内进行合并；\n\nhotFix –&gt; master\n\n生产紧急缺陷修复发布分支合并。合并时机：每次紧急版本在准生产验证通过，并上生产后的1-2天内进行合并，并且要往develop进行一次合并，均合并完成后，及时删除hotFix分支；\n\nhotFix –&gt; develop\n\n生产紧急缺陷修复同步到主开发基线的分支合并。合并时机：hotFix  master完成后，同时完成hotFix  develop，并及时删除hotFix分支；\n\ndailyFix –&gt; feature&#x2F;devx\n\n当有新需求时，基于当前的develop的送测分支拉取featrue分支，并在该分支上进行新功能、新需求的开发；\n\nFeature&#x2F;devx –&gt; develop\n\n阶段性新需求交付分支合并（建议新需求交付日期前一段时间进行该合并，为新需求合并回主开发基线留出充足时间进行合并后的稳定性及缺陷相关验证）。\n4、分支管理整体流程\n分支管理相关概述图如上，相关步骤说明如下：1）\t项目初始只有一个master分支，基于master创建develop分支作为主开发基线；2）\t开发人员在主开发基线develop上进行日常缺陷修复，代码提交；3）\t日常FAT、UAT版本发布前进行develop到dailyFix的代码同步；4）\t开发人员在主开发基线develop上进行日常缺陷修复，代码提交；5）\t日常FAT、UAT版本发布前进行develop到dailyFix的代码同步；6）\t开发人员在主开发基线develop上进行日常缺陷修复，代码提交；7）\t日常FAT、UAT版本发布前进行develop到dailyFix的代码同步；8）\t到达一个时点，进行生产上缺陷修复及需求交付，版本发布到生产；9）\t开发人员在主开发基线develop上进行日常缺陷修复，代码提交；10）\t日常FAT、UAT版本发布前进行develop到dailyFix的代码同步；11）\t开发人员在主开发基线develop上进行日常缺陷修复，代码提交；12）\t日常FAT、UAT版本发布前进行develop到dailyFix的代码同步；13）\t当生产发现紧急缺陷，基于生产版本对应的master分支创建hotFix分支，并在此分支上进行缺陷修复；14）\t紧急缺陷修复后再准生产环境进行版本验证，若为通过，重复步骤14进行缺陷修复及发布到准生产，直到缺陷完全修复并验证通过；15）\t缺陷修复并发布至生产环境后，将hotFix代码同步至master分支，保证master分支始终与生产环境保持版本一致；16）\t紧急缺陷修复代码同步至主开发基线develop分支，进行后置的FAT、UAT的验证，同时保证修复代码合并进以后的版本中；17）\t开发人员在主开发基线develop上进行日常缺陷修复，代码提交；18）\t日常FAT、UAT版本发布前进行develop到dailyFix的代码同步；19）\t与行方评审完一批新需求，根据功能、业务模块、交付时间等不同维度将需求拆分为不同维度的小需求，对应多个feature分支；这些feature分支均基于主日常出版本基线dailyFix分支创建的；20）\t开发人员在主开发基线develop上进行日常缺陷修复，代码提交；21）\t日常FAT、UAT版本发布前进行develop到dailyFix的代码同步；22）\t新需求开发人员在对应的feature&#x2F;xxx分支上进行迭代开发，代码提交，Jenkins拉取feature&#x2F;xxx本身进行版本编译，Jenkins编译版本期间，为保证编译成功tag推送成功，可通过gitlab的分支权限机制，禁止任何人推动代码到服务器，待编译并推送tag完成，开放该分支的代码推送权限，通过OMS部署到新需求验证环境进行FAT、UAT验证；23）\t新需求开发过程中需要定期将主开发基线的修复缺陷代码单向同步到新功能分支feature&#x2F;xxx中，冲突在日常开发中解决，避免最终合并回develop分支集中解决冲突；24）\t新需求开发人员在对应的feature&#x2F;xxx分支上进行迭代开发，代码提交，Jenkins拉取feature&#x2F;xxx本身进行版本编译，Jenkins编译版本期间，为保证编译成功tag推送成功，可通过gitlab的分支权限机制，禁止任何人推动代码到服务器，待编译并推送tag完成，开放该分支的代码推送权限，通过OMS部署到新需求验证环境进行FAT、UAT验证；25）\t阶段性新需求开发及初步验证完成，合并到主开发基线develop分支，进行迭代验证，该合并策略存在两种分别如下：a)\t基于Git日志的自动合并策略；b)\t人工出去代码比对合并策略；以上两种策略针对的情况不同：（a）策略为该feature&#x2F;xxx分支上的所有新需求均要交付准备上线，该情况为出现较好的情况，基于Git自动的分支合并，人工解决少量冲突即可，较为省时省力；（b）策略为该feature&#x2F;xxx分支上包含本次交付要上线的需求和由于突发因素不能上线的需求，该情况需要人工进行feature&#x2F;xxx分支抽取代码到develop的合并，人工工作量大，且容易出现漏合（某个功能没有完全合并进来）及误合（合并中包含了本次不能上线的需求的代码）等问题。26）\t开发人员在主开发基线develop上进行日常缺陷修复，代码提交；27）\t日常FAT、UAT版本发布前进行develop到dailyFix的代码同步；28）\t阶段性新需求交付合并回主开发基线迭代验证完成，合并到master分支准备发布到准生产验证完成通过后，发布到生产环境；29）\t基于当前主开发基线develop分支创建feature&#x2F;xxxx分支，作为新一轮新需求（或上一轮遗留需求）开发分支；30）\t如有上一轮需求中的遗留需求，将之前合并进develop的分支再合并到新创建的feature&#x2F;xxxx分支上，合并过程中的冲突解决策略为以新创建的feature&#x2F;xxxx为准；31）\t新需求开发人员在对应的feature&#x2F;xxx分支上进行迭代开发，代码提交，并发布到新需求验证环境进行FAT、UAT验证；32）\t新需求开发过程中需要定期将主开发基线的修复缺陷代码单向同步到新功能分支feature&#x2F;xxx中，冲突在日常开发中解决，避免最终合并回develop分支集中解决冲突；33）\t新需求开发人员在对应的feature&#x2F;xxx分支上进行迭代开发，代码提交，并发布到新需求验证环境进行FAT、UAT验证；34）\t参考（24），阶段性新需求开发及初步验证完成，合并到主开发基线develop分支，进行迭代验证，策略及选择同（24）；35）\t开发人员在主开发基线develop上进行日常缺陷修复，代码提交；36）\t日常FAT、UAT版本发布前进行develop到dailyFix的代码同步；37）\t阶段性新需求交付合并回主开发基线迭代验证完成，合并到master分支准备发布到准生产验证完成通过后，发布到生产环境；38）\t开发人员在主开发基线develop上进行日常缺陷修复，代码提交；39）\t日常FAT、UAT版本发布前进行develop到dailyFix的代码同步；\n注\n\n1、开发人员在主开发基线develop上进行日常缺陷修复，代码提交；日常FAT、UAT版本发布前进行develop到dailyFix的代码同步；与大部步骤是并行进行的，也就是说在develop上进行缺陷修改及验证是长期的过程。\n2、HotFix分支有生产紧急缺陷是创建，修复后删除该分支；\n3、所有开发分支都从develop创建；\n4、Develop分支上的代码都要整体上，在合并到develop分支前将不要上的需求滤掉；\n5、dailyFix往feature同步问题如上流程图所示，develop定期同步到feature分支，考虑到及时将develop修复缺陷状态同步到新功能分支上，将合并冲突的工作分散到日常的同步中，减小最终的feature合并到develop集中解决冲突的人工工作。\n\n","categories":["Git"],"tags":["Git-Flow"]},{"title":"Git的分支合并时非同源的几种解决方案分析","url":"/2017/09/19/git-problem-solution/","content":"\n刚毕业入职来到公司，刚好赶上版本控制SVN转Git。于是，趁着这个时机，深入了解一下Git。当然在切换使用过程中也才过许多坑，在此进行一下经验总结，以使后来者少走弯路。\n\n\n首先，介绍一下来到公司时面临的现状。由于公司是做的银行系统，需要较高的保密性，故开发在局域网内进行，应用coding.net等的在线git仓库不可行，需要到客户银行进行现场开发及公司版本的开发，故存在现场和基地两个地方的同时开发。其中，现场主要是针对行方用户测试的缺陷修改以及外围系统（支付系统、手机银行等近60个外围系统）的接入联调等；基地主要是新功能新需求的开发等。因此，需要定期将基地开发的新功能合并到现场并进行现场测试。\n\n\n\n\n接下来，在我们的第一次将基地代码合并到现场时，总共有3个项目源码，其中最大的一个项目遇到了如下问题：\n\nfatal: refusing to merge unrelated histories\n\n也就是说两部分代码是非同源的。经了解最初将基地代码部署到现场时，直接将基地的.git文件夹删除，作为一个新的没有提交历史的项目推送到了现场搭建的Git仓库。从而导致了虽然基地与现场最初的代码一致，但是没有相关的提交历史（即没有基于某一个共同的Git版本号做的后续开发）。使得基于Git的日志的合并无法进行。\n针对这个问题，我们进行了如下几种尝试：Git的强制合并进行两个分支合并时添加 –allow-unrelated-histories 参数，进行强制的diff合并。经尝试，发现项目有1680个文件删除，5884个文件新增，5117个文件修改。这就意味着我们要解决5117个文件的冲突。[:sad]\n此方案不可行，pass掉。\nGit的打补丁的方式通过将基地拿到现场最初版本到本次合并的时间段内的基地的提交通过发布补丁包的方式，再将补丁包应用到现场的代码中。\ngit -c diff.mnemonicprefix=false -c core.quotepath=false -c credential.helper=manager-st format-patch --stdout -1 6531df71a840ab9540b88f6c85cf50c1b70be0dbgit -c diff.mnemonicprefix=false -c core.quotepath=false -c credential.helper=manager-st format-patch --stdout -1 90d424d070170dd6e2257f4b1a877e8c164aad62git -c diff.mnemonicprefix=false -c core.quotepath=false -c credential.helper=manager-st format-patch --stdout -1 feb2a8ac5788a09ad3a838d5db830c779473092bgit -c diff.mnemonicprefix=false -c core.quotepath=false -c credential.helper=manager-st format-patch --stdout -1 e06a38c3c57c902cf9abf89d125027deb6df142bgit -c diff.mnemonicprefix=false -c core.quotepath=false -c credential.helper=manager-st format-patch --stdout -1 c3e764c806bdbd7a41bf66ccdc583d9b6d8ddc08git -c diff.mnemonicprefix=false -c core.quotepath=false -c credential.helper=manager-st format-patch --stdout -1 c8ff104b5446e1dd6086aa881ee8f4d997fe359bgit -c diff.mnemonicprefix=false -c core.quotepath=false -c credential.helper=manager-st format-patch --stdout -1 50a8bbd0bf178e4c5ca0b59f49a8e91b70b4bc84...... ......\n\n生成完补丁包后，发现补丁包大小为1.67G（感觉要跪），将补丁开始应用到当前项目。\ngit -c diff.mnemonicprefix=false -c core.quotepath=false -c credential.helper=manager-st am -p 1 --3way \\patch.diff\n由于补丁包太大，尝试应用补丁不可行（工作量太大，相当于rebase操作总共322步，而且每步都要解决不同量[不可预估]的冲突）！\n通过抽取增量的方式开始尝试进行增量的抽取，将基地拿到现场最初版本到本次合并的时间段内的基地的增量抽取出来。即，只基地两个时间点的代码diff，获取基地这段时间的文件变更列表，现场仅考虑基地有变更的文件。再通过对比工具（如：BeyondCompare）将增量列表中的文件对比合并到现场的代码中。通过抽取增量后，发现有7个文件删除、2173个文件已添加、274个文件已修改。这意味着我们要比较解决的冲突文件为274个文件，这比方案一要解决的冲突降低了一个数量级。\n\n经权衡后，我们选择了抽取增量的方式进行合并。\n\n其余的2个较小的项目，由于在最初到现场时保留了.git文件夹（也是奇怪，为何不是都保留或都不保留）。可以进行Git自动合并。要解决的冲突量都在30个文件以内，而且冲突文件大多数为公共文件、注册文件等。故工作量时在可接受范围内。\n\n经验教训，将源代码部署到一个新的环境进行两地各自局域网内同时开发时，需要合并代码的一定要保留.git文件夹，保留之前的提交记录、版本号等。\n\n","categories":["经验积累"],"tags":["Git"]},{"title":"全排列之康托编码(附 Java版代码)","url":"/2016/08/21/CantorCode/","content":"\n引言：\n\n最近准备校招，开始重拾算法，开启leetcode刷题之旅。刷到#60 Permutation Sequence 一题，憋了半天，开始Google，于是发现一个新奇的算法——康托编码。\n康托展开：全排列到一个自然数的双射X&#x3D;an!+an-1!+…+ai!+…+a[1]0! ，其中a[i]为当前未出现的元素中是排在第几个（从0开始）。这就是康托展开。康托展开可用代码实现。(使用范围：没有重复元素的全排列)\n\n\n全排列的编码{1,2,3,4,…,n}的排列总共有n!种，将它们从小到大排序，怎样知道其中一种排列是有序序列中的第几个？如：{1, 2, 3}的数组按照从小到大的全排列为：123, 132, 213, 231, 312, 321。现在，要知道231是第几大的数。分析如下：第一位是2，比2小的数有1，第一位是1的全排列个数是 2! 个，故由第一位能确定 1 × 2! &#x3D; 2 个比 231 小的数；第二位是3，比 3 小的数有1、2，而2在第一位，故由前两位确定的比231小的数有 1 × 1! &#x3D; 1 个；最后1位为1，1小的数有0个。最终确定的比231小的数的个数为1 × 2! + 1 × 1! &#x3D; 3 个，而231为第4大的数。\n再举个列子：1324是{1, 2, 3, 4}排列数中第几大的数：第一位为1,小于1的数有0个，故为0 × 3! &#x3D; 0个；第二位为3，小于3的数有1、2，其中1在第一位，故为1 × 2! &#x3D; 2 个；第三位为2，小于2的数有1，而1在第一位，故为 0 × 1! &#x3D; 0 个；第四位为4，小于4的数有1、2、3，而1在第一位，3在第二位，2在第三位，故为0 × 0! &#x3D; 0个；所以比1324小的排列有0 3! + 1 2! + 0 1! + 0 0! &#x3D; 2 个，而1324是第三大的数。又例如，排列2 6 3 8 1 4 9 7 5展开为61683，因为X&#x3D;1 8! + 4 7! + 1 6! + 4 5! + 0 4! + 0 3! + 2 2! + 1 1! &#x3D; 61683.\nJava实现如下：\n/** * 计算给定数是全排列中第几大的数 * @param n 全排列位数 * @param m 给定的数 * @return */public int KT(int n, int m) &#123;    String mStr = String.valueOf(m);    int result = 1;    if (n &lt; 2) return result;    int[] level = new int[n + 1];  // 缓存阶乘结果    level[0] = 1;    for (int i = 1; i &lt;= n; i++) level[i] = level[i - 1] * i;    // 康托计算    boolean[] flag = new boolean[n + 1];    for (int i = 0; i &lt; n; i++) &#123;        int cur = mStr.charAt(i) - &#x27;0&#x27;;        int count = cur - 1;        for (int j = 1; j &lt; cur; j++) &#123;            if (flag[j]) count--;        &#125;        flag[cur] = true;        result += count * level[n - i - 1];    &#125;    return result;&#125;\n\n全排列的解码如何找出第16个（按字典序的）{1,2,3,4,5}的全排列？过程如下：\n首先用16-1得到15用15去除4! 得到0余15用15去除3! 得到2余3用3去除2! 得到1余1用1去除1! 得到1余0有0个数比它小的数是1，所以第一位是1有2个数比它小的数是3，但1已经在之前出现过了所以是4有1个数比它小的数是2，但1已经在之前出现过了所以是3有1个数比它小的数是2，但1,3,4都出现过了所以是5最后一个数只能是2\n所以排列为1 4 3 5 2\nJava实现如下：\n/** * 给定全排列位数和数字K，返回全排列中第 K 大的数 * @param n 全排列位数 * @param k 给定的数K * @return */public int Cantor(int n, int k) &#123;    if (n &lt; 1) return -1;    int[] level = new int[n];  // 缓存阶乘结果    level[0] = 1;    for (int i = 1; i &lt; n; i++) &#123;        level[i] = level[i - 1] * i;    &#125;    int result = 0;    boolean[] flag = new boolean[n + 1];    k--;    for (int i = 0; i &lt; n; i++) &#123;        int cur = k / level[n - i - 1];        int j = 1;        // 从头开始找到 cur 个坑位为尚未填写的数字        for (; j &lt;= n; j++) &#123;            if (!flag[j]) &#123;                if (cur == 0)   break;                --cur;            &#125;        &#125;        flag[j] = true;        result = result * 10 + j;        k = k % level[n - i - 1];    &#125;    return result;&#125;","categories":["算法"],"tags":["算法"]},{"title":"Hello World","url":"/2016/08/19/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\n\n\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"title":"面试中可能问到的Git问题","url":"/2017/10/23/git-learn/","content":"\n团队协作能力一直是我们招聘开发人员的重要考核指标之一。而考核这个能力的原因很简单：一般公司都不会只有一个开发…而一旦涉及多人协作开发，良好的协作能力和习惯能显著提高整个团队的开发效率。Time is money！\n\n说到协作，面试中当然就会聊到开发人员日常最需要协作的事情，代码协作。因为 Github 在国内的流行，很多公司都已经把代码托管到 Github 或者内部的 git 服务上，所以大家也慢慢把 git 技能的考察引入到面试中。\n\n\n\n\n\n下面就分享一些笔者个人整理的 git 相关问题以及解析。\n\n基础部分平时都用什么Git工具？命令行：只能说十个里面九个菜，还有一个是大神，虽然命令行提供了全部的功能，但是很多用 GUI 工具可以很便捷解决的问题，命令行做起来都比较麻烦。当然并不是让大家不要去命令行，通过命令行可以对 git 的功能和原理有一个更深入的了解。\n由于命令行上手较慢，所以我们通常会用一些第三方 GUI 工具来提高我们 git 仓库管理的效率：\n\n作为Java开发，接触最多的当然是Intellij IDEA，内置了 Git 的功能。它是真的好用，只需简单配置，即可轻松使用。\nSourceTree：笔者日常使用的一个图形化的 git 增强工具，而最好用的功能就在于它集成了 GitFlow，让开发者可以更简单、更规范的去做一些 git 操作；另外它还提供了更友好的 merge 界面，但是操作起来不是很顺手，因为它只支持整行删除;\n\n\n\nSmartGit：\n\n\n\nAtom：Atom 本身并不是专门用来做 git 管理的工具，而是一个支持多种开发语言的开源 IDE。提到它的原因是 merge-conflicts, 这个插件提供的 merge 界面，要比 SourceTree 的更好用，Atom 会在当前内容的基础上，把有冲突的部分直接对比标示出来，开发人员可以像编辑普通文本一样在标示的区域内直接进行修改，并最终选择自己满意的那个部分作为 merge 之后的内容。\n\nVSCode：如同Atom一样，VSCode生来就内置了基本的Git工具，日常的操作基本可以通过页面简单的操作来完成。\n\n\n\n而且配合其插件Git Lens，在写作开发中更是得心应手。它能让你清楚的知道文件的每一行是由谁改动的，是不是很强大。\n\ngit add 和 git stage 有什么区别在回答这个问题之前需要先了解 git 仓库的三个组成部分：工作区（Working Directory）、暂存区（Stage）和历史记录区（History）：\n\n工作区：在 git 管理下的正常目录都算是工作区，我们平时的编辑工作都是在工作区完成；\n暂存区：临时区域。里面存放将要提交文件的快照；\n历史记录区：git commit 后的记录区。\n\n然后是这三个区的转换关系以及转换所使用的命令：\n然后我们就可以来说一下 git add 和 git stage 了。其实，他们两是同义的，所以，惊不惊喜，意不意外？这个问题竟然是个陷阱…引入 git stage 的原因其实比较有趣：是因为要跟 svn add 区分，两者的功能是完全不一样的，svn add 是将某个文件加入版本控制，而 git add 则是把某个文件加入暂存区，因为在 git 出来之前大家用 svn 比较多，所以为了避免误导，git 引入了git stage，然后把 git diff –staged 做为 git diff –cached 的相同命令。基于这个原因，我们建议使用 git stage 以及 git diff –staged。\n考察关键点：对 git 工作区（Working Directory）、暂存区（Stage）和历史记录区（History）以及转换关系的了解；对 git add 和 git stage 的了解。\n回答关键点：工作区（Working Directory）、暂存区（Stage）和历史记录区（History）以及转换关系不能少；git stage 是 git add 的同义指令；我用 git stage。\ngit reset、git revert 和 git checkout 有什么区别这个问题同样也需要先了解 git 仓库的三个组成部分：工作区（Working Directory）、暂存区（Stage）和历史记录区（History）。\n首先是它们的共同点：用来撤销代码仓库中的某些更改。\n然后是不同点：\n首先，从 commit 层面来说：\n\ngit reset 可以将一个分支的末端指向之前的一个 commit。然后再下次 git 执行垃圾回收的时候，会把这个 commit 之后的 commit 都扔掉。git reset 还支持三种标记，用来标记 reset \n\n–mixed：会影响到暂存区和历史记录区。也是默认选项；\n–soft：只影响历史记录区；\n–hard：影响工作区、暂存区和历史记录区。\n\n\n\n注意：因为 git reset 是直接删除 commit 记录，从而会影响到其他开发人员的分支，所以不要在公共分支（比如 develop）做这个操作。\n\ngit checkout 可以将 HEAD 移到一个新的分支，并更新工作目录。因为可能会覆盖本地的修改，所以执行这个指令之前，你需要 stash 或者 commit 暂存区和工作区的更改。\n\ngit revert 和 git reset 的目的是一样的，但是做法不同，它会以创建新的 commit 的方式来撤销 commit，这样能保留之前的 commit 历史，比较安全。另外，同样因为可能会覆盖本地的修改，所以执行这个指令之前，你需要 stash 或者 commit 暂存区和工作区的更改。然后，从文件层面来说：\n\ngit reset 只是把文件从历史记录区拿到暂存区，不影响工作区的内容，而且不支持 –mixed、–soft 和 –hard。\n\ngit checkout 则是把文件从历史记录拿到工作区，不影响暂存区的内容。\n\ngit revert 不支持文件层面的操作。\n回答关键点：\n对于 commit 层面和文件层面，这三个指令本身功能差别很大。\n\ngit revert 不支持文件层面的操作。\n\n不要在公共分支做 git reset 操作。\n\n\nGitFlow 基本流程和你的理解GitFlow 是由 Vincent Driessen 提出的一个 git操作流程标准。包含如下几个关键分支：\n\n\n\n名 称\n说 明\n\n\n\nmaster\n主分支\n\n\ndevelop\n主开发分支，包含确定即将发布的代码\n\n\nfeature\n新功能分支，一般一个新功能对应一个分支，对于功能的拆分需要比较合理，以避免一些后面不必要的代码冲突\n\n\nrelease\n发布分支，发布时候用的分支，一般测试时候发现的 bug 在这个分支进行修复\n\n\nhotfix\nhotfix 分支，紧急修 bug 的时候用\n\n\nGitFlow 的优势有如下几点：\n并行开发：GitFlow 可以很方便的实现并行开发：每个新功能都会建立一个新的 feature 分支，从而和已经完成的功能隔离开来，而且只有在新功能完成开发的情况下，其对应的 feature 分支才会合并到主开发分支上（也就是我们经常说的 develop 分支）。另外，如果你正在开发某个功能，同时又有一个新的功能需要开发，你只需要提交当前 feature 的代码，然后创建另外一个 feature 分支并完成新功能开发。然后再切回之前的 feature 分支即可继续完成之前功能的开发。\n协作开发：GitFlow 还支持多人协同开发，因为每个 feature 分支上改动的代码都只是为了让某个新的 feature 可以独立运行。同时我们也很容易知道每个人都在干啥。\n发布阶段：当一个新 feature 开发完成的时候，它会被合并到 develop 分支，这个分支主要用来暂时保存那些还没有发布的内容，所以如果需要再开发新的 feature，我们只需要从 develop 分支创建新分支，即可包含所有已经完成的 feature 。\n支持紧急修复：GitFlow 还包含了 hotfix 分支。这种类型的分支是从某个已经发布的 tag 上创建出来并做一个紧急的修复，而且这个紧急修复只影响这个已经发布的 tag，而不会影响到你正在开发的新 feature。\n\n然后就是 GitFlow 最经典的几张流程图，一定要理解：\nfeature 分支都是从 develop 分支创建，完成后再合并到 develop 分支上，等待发布。\n当需要发布时，我们从 develop 分支创建一个 release 分支\n然后这个 release 分支会发布到测试环境进行测试，如果发现问题就在这个分支直接进行修复。在所有问题修复之前，我们会不停的重复发布-&gt;测试-&gt;修复-&gt;重新发布-&gt;重新测试这个流程。\n发布结束后，这个 release 分支会合并到 develop 和 master 分支，从而保证不会有代码丢失。\nmaster 分支只跟踪已经发布的代码，合并到 master 上的 commit 只能来自 release 分支和 hotfix 分支。\nhotfix 分支的作用是紧急修复一些 Bug。\n它们都是从 master 分支上的某个 tag 建立，修复结束后再合并到 develop 和 master 分支上。\n\n考察关键点\nGitFlow 包含的分支类型和功能；\n\nGitFlow 的优势；\n\n对 GitFlow feature、release、hotfix 流程的理解。\n回答关键点\nGitFlow 的基本内容以及优势；\n\n对于 feature 流程，都是从 develop 分支发起，然后通过 PR／MR 的方式合并回 develop 分支；\n\n对于 release 流程，则是要注意几点：\n\n如果 release 分支上有 bug 需要修复，直接在 release 分支上完成；\nrelease 分支上的 bug 修复要持续通过 PR／MR 的方式合并回 develop 分支；\n最后确认发版的时候才把 release 分支直接合并到 master 分支。\n\n\n对于 hotfix 流程，则是要注意几点：\n\n从 master 分支发起；\n修复完要同时合并到 develop 和 master。\n\n\n\n","tags":["Git"]},{"title":"Git常用命令一览表","url":"/2017/09/03/git-sheet/","content":"\nGit常用命令列表，以备快速查阅使用！\n\n\n\n创建初始化## clone an existing repository（克隆一个Git仓库到本地）git clone http://user@domain.com/repo.git##Create a new local repository(初始化一个Git本地仓库)git init\n\n本地有改动时##Changed files in your working directory 除了问题时，先用该命令查看本地状态git status##Changes to tracked files 对比加入版本控制的文件git diff##Add all current changes to the next commit 将本地的所有变更提交到暂存区git add .##Add some changes in &lt;file&gt; to the next commit 同一个文件进行多次变更并且想要记录每次变更的提交时会用到该命令git add -p &lt;file&gt;##Commit all local changes in tracked files 提交所有本地加入版本控制的所有变更到本地git commit -a##Commit previously staged changes 提交放入暂存区的变更git commit##Change the last commit 修改没有推送到远端的commit注释##Don‘t amend published commits! 已经推送的提交信息永远不能更改了，不要尝试amend了！git commit --amend\n\n提交历史相关##Show all commits, starting with newest 查看提交历史git log##Show changes over time for a specific file 查看某一个文件的提交历史git log -p &lt;file&gt;##Who changed what and when in &lt;file&gt; 查看一个文件的提交历史git blame &lt;file&gt;\n\n分支及标签相关##List all existing branches 查看所有存在的分支（包含远程分支）git branch -av##Switch HEAD branch 切换分支（本质就是变更HEAD头指针，顺便更新该项目下的文件为相应分支的状态）git checkout &lt;branch&gt;##Create a new branch based on your current HEAD 基于当前分支的状态创建新分支git branch &lt;new-branch&gt;##Create a new tracking branch based on a remote branch 拉取新的远程分支到本地git checkout --track &lt;remote/branch&gt;##Delete a local branch 删除本地分支（不能删除当前所在的分支）git branch -d &lt;branch&gt;##Mark the current commit with a tag 基于当前所在的分支及版本打标签git tag &lt;tag-name&gt;\n\n提交本地及推送远程##List all currently configured remotes 查看所有远端的列表git remote -v##Show information about a remote 查看某一远端的信息git remote show &lt;remote&gt;##Add new remote repository, named &lt;remote&gt; 添加一个远端仓库git remote add &lt;shortname&gt; &lt;url&gt;##Download all changes from &lt;remote&gt;, but don‘t integrate into HEAD 获取远端的更新记录，但是不改变当前的HEAD指针（即，不对本地文件内容做更新修改）git fetch &lt;remote&gt;##Download changes and directly merge/integrate into HEAD 获取远端的更新记录，并应用到本地git pull &lt;remote&gt; &lt;branch&gt;##Publish local changes on a remote 向远端推送本地提交（commit）的变更git push &lt;remote&gt; &lt;branch&gt;##Delete a branch on the remote 删除远端仓库的一个分支git branch -dr &lt;remote/branch&gt;##Publish your tags 推送标签到远端（注：我们的gitblit中没有开放开发人员的推送tag权限）git push --tags\n\n合并##Merge &lt;branch&gt; into your current HEAD 将指定的分支合并到当前分支上git merge &lt;branch&gt;##Rebase your current HEAD onto &lt;branch&gt; 以rebase的方式合并分支##Don‘t rebase published commits! 注：不要已rebase的方式合并已推送到远程的提交git rebase &lt;branch&gt;##Abort a rebase 终止rebase合并git rebase --abort##Continue a rebase after resolving conflicts 在rebase合并过程中可能出现冲突，在解决冲突后进行继续rebase方式的合并git rebase --continue\n\n重置与回滚##Discard all local changes in your working directory 丢弃本地加入到版本控制的所有变更git reset --hard HEAD##Discard local changes in a specific file 将丢弃指定文件的本地未提交的变更git checkout HEAD &lt;file&gt;##Revert a commit (by producing a new commit with contrary changes) 生成一个新的提交来撤销某次提交，此次提交之前的commit都会被保留git revert &lt;commit&gt;##Reset your HEAD pointer to a previous commit 回到某次提交，提交及之前的commit都会被保留，但是此次之后的修改都会被退回到暂存区##…and discard all changes since then 丢弃本地所有修改的reset模式git reset --hard &lt;commit&gt;##…and preserve all changes as unstaged changes 仅用HEAD指向的目录树重置暂存区，工作区不会受到影响git reset &lt;commit&gt;##…and preserve uncommitted local changes 仅用HEAD指向的目录树重置暂存区，保留之前工作区未提交的变更git reset --keep &lt;commit&gt;","tags":["Git"]},{"title":"我的面经","url":"/2016/08/29/%E6%88%91%E7%9A%84%E9%9D%A2%E7%BB%8F/","content":"2016年底的一个学渣的校招面试经验！\n\n\n[0813] - 搜狐研发中心预约9：00,结果迟到20分钟。先现场笔试，后面试。一面：先看了下我的笔试情况，后针对题目部分细节问了下原理。还算比较轻松。后看了下算法，说我第一题没有达到时间复杂度要求。看了下确实有时间复杂度要求，然后说用二分法。开始介绍项目，blabla…….其中问到redis集群如何负载均衡。然后问实现mybatis的dao部分该怎么实现，动态代理生成实现类；防止Sql注入怎么处理（PrepareStatment并没有答上）。然后问spring两大核心，IoC和AOP的实现原理，blabla……\n二面：一直在问项目的问题，有问到数学建模竞赛跟中间件比赛的大概情况，然后让我画一下在搜狗的系统平台架构图。问想做什么方向……\n[0814] - 京东广告部预约10：00,还好提前5分钟到。先等级，然后带到会议室做笔试题，每人给40分钟时间。后带到侯面区准备面试。一面：面试官先看了下笔试答题情况，竟然说我笔试答的还可以。看了下我的简历，介绍下简历中的那个C 项目，blabla…….中间打断我问了几次，然后面试官大概清楚了后说这个项目如果让你用java实现一下，你想下能用到什么数据结构及JDK的相关库或工具。然后给我一张纸让我试着写一下……多线程相关问题：threadLocalJ.U.C下的包用到过那几个？concurrentHashMap、blokingQueue、Atomicxxxxx相关、Lock&#x2F;UnLock、ReentrantLock又问concurrentHashMap与HashTablle的区别MySQL的几种引擎介绍下。MyISAM和InnoDB，两者主要区别是：虽然都是B+树实现的索引，但是MyISAM在叶子节点存的是引用，真实数据不是顺序存放，而InnoDB 在叶子节点存的是数据，在进行范围查找时InnoDB 性能要高于前者。乐观锁、悲观锁介绍。数据库事物介绍一下。五种事物级别及相关特点脏读、不可重复读、幻读Java中防SQL注入措施？昨天面搜狐刚好问到，PrepareStatment的预编译。PrepareStatment预编译的结果存在哪里？这个还真不知道。面试官人挺好，还跟我讲解下：JDBC会先看数据库的Server是否支持本地编译缓存，如果支持则会将预编译的结果放到数据库Server端；如果不支持则会在java服务器端保存预编译结果。当然前者好处很是明显了。Filter的生命周期，Filter和Listener的区别。Spring的bean加载机制介绍一下。SpringMVC接受数据处理大致过程。一面从11：00一直面到12：40左右问的比较多，还有部分问题一时想不起来了，后续想到再补充。\n二面：下午1：00问项目闲聊。\n[0820] - 华为清北专场招聘会招聘会前会多次短信及邮件通知，并华为也会要求收集相应反馈信息。\n一面：面试官先看了下我的简历，你本科是学数学的，看你研究生恶补了不少计算机的东西啊，blabla…….现在选三个点：你的数学比较擅长的点；项目中比较擅长的点；自己想表现的点。各找一个大概说下吧。一时没太反应过来，面试官说先来数学，这样吧，写一个泰勒展开式吧。e^x的展开写一下。出来后发现多加了一个 (-1)^i 。又出一道有关递归画图的算法题，大概想了下5分钟出了一版。大概解释了下。感觉一面官对我的数学背景有些兴趣。\n二面：面试官看了下我的简历问大数据的项目的优化部分，问实现项目部分，问实验室项目部分，问中间件比赛部分。然后聊工闲聊。\n[0831] - 搜狗先做了10道笔试题，后来貌似看打我得分为6.也就是错了4题浏览简历，看到我简历上有参加阿里巴巴的中间件挑战赛，然后问简历项目上没有写这段经历啊。然后面试官就开始跟我聊这个点，因为当时做了大部分工作，所以聊的还是比较轻松的，问的一些细节都基本答上来。算法及实现题3道：一个数组先递增后递减，找到最大的数（个人感觉考察实现细节）、两个链表查看是否有公共节点并找出第一个公共节点、单链表的逆置然后问HashMap 与 ConcurrentHashMap 的区别、HashMap 出现ConcurrentModificationException 异常是什么情况下、JVM内存模型、volatile 的JVM实现原理、synchronized 的特征、JVM垃圾收集算法、若一个Java程序运行后占用系统资源一直居高不下，怎么排查原因shell：如何查看系统资源占用、如何查看上一条命令是否执行成功数据库：MySQL数据库引擎及区别、MySQL事物默认事物隔离级别、分别介绍数据库的几种事物隔离级别、数据库索引的原理、xxx where b&#x3D;xxxx and c in () 与 xxx where c in () and b&#x3D;xxx 这两个那一个会查索引（有些蒙逼）操作系统：线程与进程的区别、操作系统管理内存的方式有哪些及各自的特点网络：TCP的三次握手及四次挥手过程、并画出每一步请求的状态转换\n[0908] - 搜狗二面算法：数字到Excel列名的转换，即1 -&gt; A , 26 -&gt; Z, 27 -&gt; AA ……时间有些久了，记不清别的题目了，只记得最后的一个：代码实现生产者消费者的生产消费过程下次面完一定及时写面经。\n[0912] - 京东校招二面简单设计仓库仓储数据库及相关后端架构算法：两个List，list中都是Point坐标点，计算一个list中的点到另一个list中各坐标点最近的10对点（计算、中间缓存及结果集的设计）中间几个问题忘记了。。。算法导论：100米的树，1米售价为a1，2米售价为a2 ……请问怎么切割这100米达到最大收益图的深度优先遍历及广度优先遍历 用到的数据结构拓扑排序的具体实现\n[0918] - 宜信技术研发中心一面：自我介绍，介绍项目……介绍项目中印象比较深的贡献或收获算法：求一个字符串的最长非重复字串（这个题跟面试官讨论的下相关优化点，最终的意思是通过选择或设计数据结构几乎完全避免重复的操作）OS：进程与线程的区别、虚拟内存的解释、Linux的iNode相关概念、软链接的实质线程安全的单例，并解释为何线程安全实际场景设计题：web服务器与DB服务器的交互性能与数据完整性的保证\n二面：闲聊，基本没有问我问题都是问我有什么问题。跟面试官开启扯淡模式。\n[0920] - 小米面试基本问的项目，算法让写了个冒泡排序问到协同过滤\n[0921] - 搜狐二面 + 终面\n介绍项目…….问到了Storm中的task，worker，executor的功能介绍及区别；一个链表，随机返回链表的一个节点值，只能遍历一遍（并证明取每个点是等概率的）；基于一个随机数发生器（等概率产生1、2），构造一个产生3、5、7为等概率的随机数发生器；再构造一个生成1到n等概率的随机数发生器\n[0921] - 网易游戏电面自我介绍，介绍项目；RockerMQ怎么实现的，Redis是多线程的么；Spring的IOC是什么；数据库引擎：MySQL的MyISM和InnoDB的比较，索引实现，事物，锁级别（行级锁，表锁）；OS：进程与线程的区别；常见的网站攻击模式及说下如何预防：SQL注入、XSS、泛洪攻击；设计一个秒杀系统的关键的几个点介绍下；在部门中产品与开发出现意见不合，你会怎么处理？\n[0924] - 百度校招面试一面二面基本都是问的项目，一面问道H2内存数据库要实现分布式，给出一个解决方案；二面让写了下快排；别的基本都是介绍项目时问到相关问题；三面：一个数组找出其中的最大最小值，实现最小的比较次数（代码实现）；100盏灯，编号1-100；100个人，编号1-100。编号为K的人会将k的倍数的编号的灯状态置反；问所有人经过后最后亮着几盏灯；随机数发生器问题，大同小异，不再多说。\n[0927] - 腾讯校招介绍项目…….concurrentHashMap的实现原理，Java8对此的变动；Java的类加载器介绍，contextClassLoader是什么；JavaWeb: Get和Post的区别；设计一个内存管理机制管理4G内存的申请与释放，给出解决方案；海量文章及一个很大的敏感词库，如何快速将敏感词找出并进行相应的替换。\n[1009] - 乐视一面：介绍项目…….Java 集合HashMap 与 HashTableHashMap 线程不安全举例 [多线程下的死循环，导致高CPU占用]ConcurrentHashMapJVM内存划分\n二面：OSI七层介绍物理层、数据链路层、网络层、传输层 的 传输单位传输层的协议有哪些套接字的种类：流式套接字(面向链接)、数据包式套接字(无链接)、原始套接字(可操控底层协议)浏览器输入 http://www.baidu.com 后从操作系统和网络两个层面介绍这背后发生了什么，尽量详细算法：手写非递归的二分查找1000g大小的单词数据，给定一个1g内存2核CPU3T硬盘，找出1000G数据中出现次数top100的单词。给出解决方案 - [MapReduace思想]数据库联合索引a、b、c 查询 where a&#x3D;x and b&#x3D;xx and c&#x3D;xxx 能命中么？为什么MySQL数据库引擎，MyISM和InnoDB的区别InnoDB 什么情况下使用行级锁 - [在索引命中的情况下使用行级锁]\n[1010] - 网易游戏 - 地点：广州先做笔试题，笔试题范围比较广Java基础，JVM内存结构及相互关系，Spring事物的传播行为及事物的隔离级别，算法：10亿IP地址的文件，找出出现次数top100的；无序数组找出最长的连续序列。面试：针对笔试题情况展开问。数据库的引擎，索引；数据库的设计，目录树的数据库表怎么设计；如何解决问题，如何学习一门新语言。\n[1016] - 美团 （霸面）一面：聊项目，中间件挑战赛中JStorm的拓扑图的设计及考虑因素；Java中集合用到过哪些？算法：给定一个目录及一个匹配字符串\n二面：天池比赛的相关内容。数据库：成体系的介绍数据库的事务隔离级别（从数据库引擎到索引到事务隔离级别及数据库锁的相关实现）Java基础：JDK原生类库中定义为final的类列举出5种；int 和 Integer 两个都能用的场景怎么抉择选择哪一个（Integer 有 null 属性，一般在数据库对应的实体类为Integer而不是int，RPC调用的参数一般用Integer）\n[1018] - FreeWheel签到，看到我的笔试成绩74，大概排在前几一面：半小时中文面，半小时英文面\n二面：hashmap实现该从哪几方面设计…之后的问题忘记了（隔两个周后写的，基本忘记了）\n[1018] - 美团终面hr面：聊得挺high，毕竟HR面跪了那么多次有点经验了终面：聊项目，具体问了天池大赛的相关内容；美团外卖从客户下单到拿到外卖能控制在35分钟左右，可以通过什么方案把时间从35分钟缩到25分钟 — 发散思维思考解决方案\n[1019] - 瓜子二手车电面算法：指定二维空间上（Xmin, Ymin）至 （Xmax, Ymax）随机分布一系列的坐标点（x,y），查出给定范围内(x1,y1)至（x2,y2）的坐标点集合先是交流解决方案，最终确定四叉树的解决方案，发我一个网址打开网页开始在线coding……隔天通知去二面，一来感觉跑去太麻烦(分两次面太不人性化了)，二来当天有事学校发三方各种事。然后没有去面。\n","categories":["面经"],"tags":["面经"]}]